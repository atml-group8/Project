{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ByteNet TF2 Notebook",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKUF2Iykev1",
        "colab_type": "text"
      },
      "source": [
        "# TF2 ByteNet\n",
        "This is a notebook that shows an example implementation of the ByteNet architecture in TensorFlow2. It has the flow of the [NMT tutorial](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb#scrollTo=TNfHIF71ulLu) from the TensorFlow website. Comments and explanatory notes are provided. Note that this notebook, or the TF2 version of ByteNet was not used for experiments by the group. Experiments were done with the PyTorch implementations instead. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leUdXX2Il7Xd",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Path Definitions \n",
        "\n",
        "Add the necessary imports and path definitions here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUSfo7y6x_6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Imports { form-width: \"150px\" }\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9w1_o97o5jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Define Paths { form-width: \"150px\" }\n",
        "MOUNT_PATH = # DEFINE MOUNT PATH HERE\n",
        "WORK_DIR = MOUNT_PATH + # DEFINE WORK_DIR\n",
        "CKPT_DIR = WORK_DIR + #DEFINE CKPT_DIR\n",
        "MODEL_DIR = WORK_DIR + #DEFINE MODEL_DIR\n",
        "DATA_DIR = WORK_DIR + #DEFINE DATA_DIR\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlUmnBGpvRQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Mount Drive { form-width: \"150px\" }\n",
        "from google.colab import drive\n",
        "drive.mount(MOUNT_PATH, force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WrSj8Z9zut6",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing\n",
        "ByteNet does dynamic unfolding. This can be done when the data is being read in.\n",
        "\n",
        "Inputs are also cleaned and processed to normalise Unicode duplicate representations. Use NFKC as opposed to NFD, because you don't want to decompose umlauts and accents. <br> \n",
        "Regex taken from [here](https://stackoverflow.com/questions/20690499/concrete-javascript-regex-for-accented-characters-diacritics) to make sure input does contain language characters you wouldn't see in Latin-based languages. Start of sequence (SOS), end of sequence (EOS) and padding char (used in dynamic unfolding) are also defined here. <br>\n",
        "Strings are made into lowercase to reduce dimensionality of character representation, because in this example notebook, we are using small amounts of data.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpwQWGh-vaNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Unicode to ASCII and Preprocess { form-width: \"150px\" }\n",
        "\n",
        "SOS_CHAR = \"\\1\"\n",
        "PAD_CHAR = \"\\2\"\n",
        "EOS_CHAR = \"\\3\"\n",
        "\n",
        "# Converts the unicode to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  \n",
        "  # NFKC - represent characters that have the same meaning\n",
        "  # with the same representation in canonical form\n",
        "  return ''.join(c for c in unicodedata.normalize('NFKC', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# character level translation - don't actually need to do the\n",
        "# kind of preprocessing word level tutorials do\n",
        "def preprocess_sentence(w, unfold=False, unfold_rate=1.2):\n",
        "  \n",
        "  # get ascii to represent chars with the same meaning the same way\n",
        "  # make it lower, because we are using smaller datasets, and make\n",
        "  # it easier to learn\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "\n",
        "  # need to standardise - some sentences have a space between \n",
        "  # last char and punctuation, and some do not\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  # and accented characters\n",
        "  accentedCharacters = \"àèìòùÀÈÌÒÙáéíóúýÁÉÍÓÚÝâêîôûÂÊÎÔÛãñõÃÑÕäëïöüÿÄËÏÖÜŸçÇßØøÅåÆæœ\"\n",
        "  accentedCharacters = unicode_to_ascii(accentedCharacters)\n",
        "  w = re.sub(r\"[^a-zA-Z\"+accentedCharacters+\"?.!,¿]+\", \" \", w)\n",
        "  w = w.strip()\n",
        "  \n",
        "  if unfold:\n",
        "    l = float(len(w))* (unfold_rate -1 ) \n",
        "    pad = PAD_CHAR * int(l)\n",
        "    w = SOS_CHAR + w + pad + EOS_CHAR\n",
        "  else:\n",
        "    w = SOS_CHAR + w + EOS_CHAR\n",
        "\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoThYcHfYcGe",
        "colab_type": "text"
      },
      "source": [
        "For reasons to normalize, look at this [blogpost](https://withblue.ink/2019/03/11/why-you-need-to-normalize-unicode-strings.html). This unit test illustrates the effect of the unicode_to_ascii function we just defined and why it is important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA-_Ona-Om_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Unicode to Ascii Sanity Check { form-width: \"150px\" }\n",
        "a = '\\u0065\\u0301'\n",
        "print(a)\n",
        "b = '\\u00e9'\n",
        "print(b)\n",
        "print(a==b)\n",
        "c = unicode_to_ascii(a)\n",
        "print(c)\n",
        "d = unicode_to_ascii(b)\n",
        "print(d)\n",
        "print(c==d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXbbaIzRxwCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Preprocess Sanity Check { form-width: \"150px\" }\n",
        "\n",
        "en_sentence = u\"3 Q. Are the four beasts limited to individual beasts , or do they represent classes or a orders ?\"\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "\n",
        "print(preprocess_sentence(en_sentence,True))\n",
        "print(preprocess_sentence(sp_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-KMcYS1o7SH",
        "colab_type": "text"
      },
      "source": [
        "## Creating Dataset\n",
        "\n",
        "Here, we create a dataset that the model can use from text files. We get the text from the files, and preprocess using the methods used above. <br>\n",
        "Using TensorFlow's inbuilt tokenizers, we convert input from string format to a tensor representation. <br>\n",
        "Finally, we create a dataset object\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQTTMA-7wkxt",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Create Dataset { form-width: \"150px\" }\n",
        "\n",
        "\n",
        "# Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples, start=1000):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = []\n",
        "\n",
        "  for num in range(num_examples):\n",
        "    pair = []\n",
        "    l = lines[start+num]\n",
        "    w = l.split('\\t')\n",
        "    pair.append(preprocess_sentence(w[0], unfold=True))\n",
        "    pair.append(preprocess_sentence(w[1], unfold=False))\n",
        "    word_pairs.append(pair)\n",
        "  \n",
        "  # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[start:start + num_examples]]\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCg6Dpy0xgTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Create Dataset Sanity Check { form-width: \"150px\" }\n",
        "ENG_FRA_PATH = DATA_DIR + \"eng-fra.txt\"\n",
        "en, fr = create_dataset(ENG_FRA_PATH, 10, 1000)\n",
        "print(en)\n",
        "print(fr)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7utaJtPyjtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngbDfxn8qlqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Tokenizing Methods { form-width: \"150px\" }\n",
        "\n",
        "\n",
        "def tokenize(lang):\n",
        "  \n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='', char_level=True)\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  # lang_tokenizer = add_special_tokens(lang_tokenizer)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  \n",
        "  # for creating dataset object\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  \n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPleiv9QqrGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Load Dataset Method { form-width: \"150px\" }\n",
        "def load_dataset(path, num_examples=None, start_pos=1000):\n",
        "  # creating cleaned input, output pairs\n",
        "  inp_lang, targ_lang = create_dataset(path, num_examples,)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6rYqH43-llp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Load Dataset Method Sanity Check { form-width: \"150px\" }\n",
        "\n",
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 1000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(ENG_FRA_PATH, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roE1wV_Q0tLR",
        "colab_type": "text"
      },
      "source": [
        "The next two cells just visualize what the tokenizer does"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvWFdaseXI96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(f\"{t} ---> {lang.index_word[t]}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzxnGqR1fRUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp8PVw5t0yEe",
        "colab_type": "text"
      },
      "source": [
        "The paper uses batches and maps input tensors (that have been padded to the nearest multiple of 50) to buckets of the same length, that are then trained on different GPUs. In this small example notebook, because of our limited hardware resources (Colab only uses 1 GPU), I am not coding this, because it has no practical advantage. Instead, given the small dataset, I'm training the network one input tensor at a time. If a bigger dataset is used, the batch size can be increased to make training quicker. <br>\n",
        "All of the dataset is used for training in this example notebok, as opposed to the standard training-validation-test split. In the evaluation section, we show the results of applying the model to a single unseen sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLg-jY9vgDkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Create Dataset Object { form-width: \"150px\" }\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor)\n",
        "BATCH_SIZE = 1\n",
        "steps_per_epoch = len(input_tensor)//BATCH_SIZE\n",
        "\n",
        "hidden_units = 20\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor))\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDmoIuLFAbkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38qXufSA4N_K",
        "colab_type": "text"
      },
      "source": [
        "## Model \n",
        "Start creating the model componenets for ByteNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNkZvS4sgKBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Residual Block { form-width: \"150px\" }\n",
        "\n",
        "class ResBlk(tf.keras.layers.Layer):\n",
        "  def __init__(self, hidden_units, dilation, kernel_size, masking=False):\n",
        "    super(ResBlk, self).__init__()\n",
        "    # hidden units\n",
        "    self.hidden_units = hidden_units\n",
        "    # dilation\n",
        "    self.dilation = dilation\n",
        "    # kernel size\n",
        "    self.kernel_size = kernel_size\n",
        "    # masking \n",
        "    self.masking = masking\n",
        "\n",
        "    self.normlayer1 = tf.keras.layers.LayerNormalization()\n",
        "    self.relu1 = tf.keras.layers.ReLU()\n",
        "    # output filters = d\n",
        "    self.conv_down = tf.keras.layers.Conv1D(hidden_units, kernel_size=1)\n",
        "\n",
        "    self.normlayer2 = tf.keras.layers.LayerNormalization()\n",
        "    self.relu2 = tf.keras.layers.ReLU()\n",
        "\n",
        "    # now for masked convolution. Need padding for kernel if using masked, \n",
        "    # tf2 handles for us\n",
        "    self.padding_type = 'causal' if masking else 'same'\n",
        "\n",
        "    self.conv_masked = tf.keras.layers.Conv1D(hidden_units, \n",
        "                                              kernel_size=kernel_size,\n",
        "                                              padding = self.padding_type,\n",
        "                                              dilation_rate = dilation)\n",
        "    \n",
        "    \n",
        "    self.normlayer3 = tf.keras.layers.LayerNormalization()\n",
        "    self.relu3 = tf.keras.layers.ReLU()\n",
        "    self.conv_up = tf.keras.layers.Conv1D(2*hidden_units, kernel_size=1)\n",
        "    \n",
        "    self.block = tf.keras.Sequential([self.normlayer1, self.relu1, self.conv_down,\n",
        "                                      self.normlayer2, self.relu2, self.conv_masked,\n",
        "                                      self.normlayer3, self.relu3, self.conv_up])\n",
        "\n",
        "  \n",
        "  def call(self, x):\n",
        "    \n",
        "    o = self.block(x)\n",
        "  \n",
        "    # residual part\n",
        "    o += x\n",
        "    return o\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "falCtK8rIyES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Encoder { form-width: \"150px\" }\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, num_chars, hidden_units, kernel_size, num_blocks,\n",
        "               num_sets):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    ## store attributes\n",
        "    self.num_chars = num_chars\n",
        "    self.hidden_units = hidden_units\n",
        "    self.kernel_size = kernel_size\n",
        "    # blocks in a set, for increasing amounts of dilation\n",
        "    self.num_blocks = num_blocks\n",
        "    # number of sets\n",
        "    self.num_sets = num_sets\n",
        "\n",
        "\n",
        "    # add the different layers and block sets\n",
        "    \n",
        "    # embedding needs mask_zero=True to tell tf it is \n",
        "    # added as not a character\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=num_chars, \n",
        "                                               output_dim=hidden_units,\n",
        "                                               mask_zero=True)\n",
        "    # up conv to 2d\n",
        "    self.conv_1 = tf.keras.layers.Conv1D(2*hidden_units, kernel_size=1)\n",
        "    \n",
        "    ## sets\n",
        "    sets = []\n",
        "    for n in range(num_sets):\n",
        "      for l in range(num_blocks):\n",
        "        dilation = 1<<l \n",
        "        sets.append(ResBlk(hidden_units, dilation, kernel_size))\n",
        "    \n",
        "    self.sets = tf.keras.Sequential(sets)\n",
        "\n",
        "    # bring back down to d\n",
        "    self.conv_2 = tf.keras.layers.Conv1D(hidden_units, kernel_size=1)\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    o = self.embedding(x)\n",
        "    o = self.conv_1(o)\n",
        "    o = self.sets(o)\n",
        "    o = self.conv_2(o)\n",
        "    o = self.relu(o)\n",
        "    return o\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egvTtt29aHGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Encoder Sanity Check { form-width: \"150px\" }\n",
        "\n",
        "enc_test = Encoder(vocab_inp_size,hidden_units,3,2,2)\n",
        "# tensorflow is weird - need to reshape if putting it just one tensor.\n",
        "# However, this should not matter if using dataset object\n",
        "a = tf.reshape(input_tensor[0], [1, len(input_tensor[0])])\n",
        "enc_out = enc_test(a)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AODGbxoNGbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Decoder { form-width: \"150px\" }\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, output_size,hidden_units, kernel_size,\n",
        "               num_blocks, num_sets):\n",
        "    \n",
        "    super(Decoder, self).__init__()\n",
        "    \n",
        "    self.output_size = output_size\n",
        "    self.hidden_units = hidden_units\n",
        "    \n",
        "    # need mask zero in decoder too\n",
        "    self.embedding = tf.keras.layers.Embedding(output_size, hidden_units, \n",
        "                                               mask_zero=True)\n",
        "    \n",
        "    ## sets\n",
        "    sets = []\n",
        "    for n in range(num_sets):\n",
        "      for l in range(num_blocks):\n",
        "        dilation = 1<<l \n",
        "        # need to mask in decoder\n",
        "        sets.append(ResBlk(hidden_units, dilation, kernel_size, masking=True))\n",
        "    \n",
        "    self.sets = tf.keras.Sequential(sets)\n",
        "\n",
        "    # from 3.6 in paper - \"...one more convolution and \n",
        "    # ReLU followed by a convolution and a final softmax layer\"\n",
        "    self.conv = tf.keras.layers.Conv1D(2*hidden_units, kernel_size=kernel_size,\n",
        "                                         padding='causal')\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "    # conv to get the output_size\n",
        "    self.conv_final = tf.keras.layers.Conv1D(output_size, kernel_size=1)\n",
        "    self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "  \n",
        "  def call(self, d_pred, e_out):\n",
        "    # d_pred: previous decoder predictions\n",
        "    # e_out: encoder output \n",
        "    \n",
        "    # d_pred shape is 2d - batchsize * number of chars already decoded\n",
        "    # after embedding will be 3d - \n",
        "    # batchsize * number of chars already decoded * embedding dim\n",
        "    emb = self.embedding(d_pred)\n",
        "\n",
        "\n",
        "    # add e_out to the RIGHT of predicted vals\n",
        "    # get 2*d features\n",
        "    emb = tf.concat([emb, e_out], 2) \n",
        "    \n",
        "    o = self.sets(emb)\n",
        "    o = self.conv(o)\n",
        "    o = self.relu(o)\n",
        "    o = self.conv_final(o)\n",
        "    \n",
        "    # take the last row, the actual prediction\n",
        "    o = o[:,-1,:]\n",
        "    o = self.softmax(o)\n",
        "    \n",
        "    return o\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktdamkP2Hcth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Decoder Sanity Check { form-width: \"150px\" }\n",
        "\n",
        "d_o_start = tf.expand_dims([targ_lang.word_index[SOS_CHAR]] * BATCH_SIZE, 1)\n",
        "e_o = enc_out[:,:1,:]\n",
        "print(f\"e_o.shape = {e_o.shape}\")\n",
        "\n",
        "dec_test = Decoder(vocab_tar_size, hidden_units, 3, 5,2)\n",
        "d_o = dec_test(d_o_start, e_o)\n",
        "print(f\"d_o.shape={d_o.shape}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHnelI6yaN2K",
        "colab_type": "text"
      },
      "source": [
        "## Model Variants \n",
        "\n",
        "The paper presents \"variants\" to the standard ByteNet architecture - the residual multiplicative block (a variant of the residual block), the Recurrent ByteNet Encoder and the Recurrent ByteNet Decoder. These are shown here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfe2pBnPpzdD",
        "colab_type": "text"
      },
      "source": [
        "### Residual Block Variant\n",
        "\n",
        "The paper mentions two variants of the Residual Blocks, used for different experiments. The Residual Multiplicative Block is shown below. This is used in the decoder during the language modelling experiments by the paper (section 3.6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vs0sYsHajCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Residual Multiplicative Block { form-width: \"150px\" }\n",
        "\n",
        "\n",
        "class MU(tf.keras.layers.Layer):\n",
        "  def __init__(self, hidden_units, dilation, kernel_size, masking = True):\n",
        "    super(MU, self).__init__()\n",
        "    \n",
        "    self.hidden_units = hidden_units\n",
        "    self.kernel_size = kernel_size\n",
        "    self.dilation = dilation\n",
        "    \n",
        "    self.masking = masking\n",
        "    \n",
        "    self.padding_type = 'causal' if masking else 'same'\n",
        "\n",
        "    self.conv_masked_1 = tf.keras.layers.Conv1D(hidden_units, \n",
        "                                              kernel_size=kernel_size,\n",
        "                                              padding = self.padding_type,\n",
        "                                              dilation_rate = dilation)\n",
        "    \n",
        "    self.normlayer_1 = tf.keras.layers.LayerNormalization()\n",
        "    self.conv_masked_2 = tf.keras.layers.Conv1D(hidden_units, \n",
        "                                              kernel_size=kernel_size,\n",
        "                                              padding = self.padding_type,\n",
        "                                              dilation_rate = dilation)\n",
        "    \n",
        "    self.normlayer_2 = tf.keras.layers.LayerNormalization()\n",
        "    self.conv_masked_3 = tf.keras.layers.Conv1D(hidden_units, \n",
        "                                              kernel_size=kernel_size,\n",
        "                                              padding = self.padding_type,\n",
        "                                              dilation_rate = dilation)\n",
        "    \n",
        "    self.normlayer_3 = tf.keras.layers.LayerNormalization()\n",
        "    self.conv_masked_4 = tf.keras.layers.Conv1D(hidden_units, \n",
        "                                              kernel_size=kernel_size,\n",
        "                                              padding = self.padding_type,\n",
        "                                              dilation_rate = dilation)\n",
        "    \n",
        "    self.normlayer_4 = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    self.w1 = self.add_weight(shape=(input_shape[-1], self.hidden_units),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    self.w2 = self.add_weight(shape=(input_shape[-1], self.hidden_units),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    self.w3 = self.add_weight(shape=(input_shape[-1], self.hidden_units),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    self.w4 = self.add_weight(shape=(input_shape[-1], self.hidden_units),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    self.b1 = self.add_weight(shape=(self.hidden_units,),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    self.b2 = self.add_weight(shape=(self.hidden_units,),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    self.b3 = self.add_weight(shape=(self.hidden_units,),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    self.b4 = self.add_weight(shape=(self.hidden_units,),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    \n",
        "    def call(self, x):\n",
        "      \n",
        "      # create x1 to x4 \n",
        "      x1 = self.conv_masked_1(x)\n",
        "      x1 = self.normlayer_1(x1)\n",
        "      x1 = tf.matmul(x1, self.w1) + self.b1\n",
        "      x1 = tf.math.sigmoid(x1)\n",
        "\n",
        "      x2 = self.conv_masked_2(x)\n",
        "      x2 = self.normlayer_2(x2)\n",
        "      x2 = tf.matmul(x2, self.w2) + self.b2\n",
        "      x2 = tf.math.sigmoid(x2)\n",
        "\n",
        "      x3 = self.conv_masked_3(x)\n",
        "      x3 = self.normlayer_3(x3)\n",
        "      x3 = tf.matmul(x3, self.w3) + self.b3\n",
        "      x3 = tf.math.sigmoid(x3)\n",
        "\n",
        "      x4 = self.conv_masked_4(x)\n",
        "      x4 = self.normlayer_4(x4)\n",
        "      x4 = tf.matmul(x4, self.w4) + self.b4\n",
        "      x4 = tf.math.tanh(x4)\n",
        "\n",
        "      # elementwise multiplication\n",
        "      h1 = tf.math.multiply(x, x2)\n",
        "      h2 = tf.math.multiply(x3,x4)\n",
        "      h3 = h2+h1\n",
        "\n",
        "      h3 = tf.math.tanh(h3)\n",
        "      out = tf.math.multiply(x1, h3)\n",
        "\n",
        "      return out\n",
        "      \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "class ResMUBlk(tf.keras.layers.Layer):\n",
        "  def __init__(self, hidden_units, dilation, kernel_size, masking=True):\n",
        "    super(ResMUBlk, self).__init__()\n",
        "    # hidden units\n",
        "    self.hidden_units = hidden_units\n",
        "    # dilation\n",
        "    self.dilation = dilation\n",
        "    # kernel size\n",
        "    self.kernel_size = kernel_size\n",
        "    # masking \n",
        "    self.masking = masking\n",
        "\n",
        "    self.normlayer1 = tf.keras.layers.LayerNormalization()\n",
        "    self.relu1 = tf.keras.layers.ReLU()\n",
        "    # output filters = d\n",
        "    self.conv_down = tf.keras.layers.Conv1D(hidden_units, kernel_size=1)\n",
        "\n",
        "    self.normlayer2 = tf.keras.layers.LayerNormalization()\n",
        "    self.relu2 = tf.keras.layers.ReLU()\n",
        "\n",
        "\n",
        "    self.mu = MU(hidden_units, dilation, kernel_size, masking)\n",
        "\n",
        "    self.conv_up = tf.keras.layers.Conv1D(2*hidden_units, kernel_size=1)\n",
        "    \n",
        "    self.block = tf.keras.Sequential([self.normlayer1, self.relu1, self.conv_down,\n",
        "                                      self.normlayer2, self.relu2,\n",
        "                                      self.mu, self.conv_up])\n",
        "\n",
        "  \n",
        "  def call(self, x):\n",
        "    \n",
        "    o = self.block(x)\n",
        "  \n",
        "    # residual part\n",
        "    o += x\n",
        "    return o\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAG5UUz0qCYy",
        "colab_type": "text"
      },
      "source": [
        "### RNN Encoder and Decoder \n",
        "\n",
        "The paper mentions the recurrent variants of the standard ByteNet architecture, which are shown here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bikR5UtAqh13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Stacked Bidirectional LSTMs { form-width: \"150px\" }\n",
        "class StackedBidirectionalLSTM(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_layers, dropout):\n",
        "\n",
        "    super(StackedBidirectionalLSTM, self).__init__()\n",
        "    self.stacked_lstms = tf.keras.Sequential()\n",
        "    for i in range(num_layers):\n",
        "      self.stacked_lstms.add(tf.keras.layers.Bidirectional(\n",
        "          tf.keras.layers.LSTM(units, dropout=dropout, \n",
        "                               return_sequences=True), merge_mode='concat'))\n",
        "    \n",
        "  def call(self, x):\n",
        "    out = self.stacked_lstms(x)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNmCDHNxqohn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title RNN Encoder { form-width: \"150px\" }\n",
        "class EncoderRNN(tf.keras.Model):\n",
        "  def __init__(self, num_chars, hidden_units, num_layers, dropout=0):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.num_chars = num_chars\n",
        "    self.hidden_units = hidden_units\n",
        "    self.num_layers = num_layers\n",
        "    self.emb = tf.keras.layers.Embedding(input_dim=num_chars, \n",
        "                                               output_dim=hidden_units,\n",
        "                                               mask_zero=True)\n",
        "    \n",
        "    self.stacked_lstms = StackedBidirectionalLSTM(hidden_units, num_layers-1, \n",
        "                                                  dropout)\n",
        "    self.last_lstm_layer = tf.keras.layers.Bidirectional(\n",
        "          tf.keras.layers.LSTM(hidden_units, dropout=dropout, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True), \n",
        "          merge_mode='concat')\n",
        "    # bidirectional gives 2*hidden_units, bring down to hidden_units\n",
        "    self.out_layer = tf.keras.layers.Dense(hidden_units)\n",
        "\n",
        "  \n",
        "  def call(self, x):\n",
        "    out = self.emb(x)\n",
        "    if self.num_layers>1:\n",
        "      out = self.stacked_lstms(out)\n",
        "    \n",
        "    out, state_h, state_c, _,_ = self.last_lstm_layer(out)\n",
        "    out = self.out_layer(out)\n",
        "    \n",
        "    hidden_states = [state_h, state_c]\n",
        "    return out, hidden_states\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-89cIFqSquAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title RNN Decoder { form-width: \"150px\" }\n",
        "class DecoderRNN(tf.keras.Model):\n",
        "  def __init__(self, output_size, hidden_units, num_layers, dropout=0):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.output_size = output_size\n",
        "    self.hidden_units = hidden_units\n",
        "    self.num_layers = num_layers\n",
        "    self.emb = tf.keras.layers.Embedding(output_size, hidden_units, \n",
        "                                               mask_zero=True)\n",
        "    \n",
        "    # because of the way sequential works, need a \"first\" layer, \n",
        "    # and then the stacks, and then the \"last\" layer, \n",
        "    # since we want access to final decoder hidden state\n",
        "    self.first_lstm_layer = tf.keras.layers.LSTM(hidden_units, \n",
        "                                                dropout=dropout, \n",
        "                                                return_sequences=True, \n",
        "                                                return_state=True)\n",
        "    self.stacked_lstms = tf.keras.Sequential()\n",
        "\n",
        "    for i in range(num_layers-2):\n",
        "      self.stacked_lstms.add(tf.keras.layers.LSTM(hidden_units, \n",
        "                                                dropout=dropout, \n",
        "                                                return_sequences=True))\n",
        "    \n",
        "    self.last_lstm_layer = tf.keras.layers.LSTM(hidden_units, \n",
        "                                                dropout=dropout, \n",
        "                                                return_sequences=True, \n",
        "                                                return_state=True)\n",
        "    self.out_layer = tf.keras.layers.Dense(output_size)\n",
        "    self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "  \n",
        "  def call(self, x, e_out, dec_hidden):\n",
        "    # x is input, i.e. most recently decoded char\n",
        "    # e_out is single context from encoder - taken from encoder_output,\n",
        "    # as enc_out[:,i:i+1,:] during training and evaluating \n",
        "    # dec_hidden is hidden state to use in LSTMs\n",
        "\n",
        "\n",
        "    # if there is no more context from the encoder, \n",
        "    # we pad with zeros! \n",
        "    if e_out.shape[1] == 0:\n",
        "      e_out = tf.zeros([1, 1, self.hidden_units])\n",
        "    \n",
        "    emb = self.emb(x)\n",
        "    emb = tf.concat([emb, e_out], 2)\n",
        "    \n",
        "    out, state_h, state_c = self.first_lstm_layer(emb, dec_hidden)\n",
        "\n",
        "    if self.num_layers>2:\n",
        "      out = self.stacked_lstms(out)\n",
        "      out, state_h, state_c = self.last_lstm_layer(out)\n",
        "    \n",
        "    elif self.num_layers==2:\n",
        "      out, state_h, state_c = self.last_lstm_layer(out)\n",
        "    \n",
        "    hidden_states = [state_h, state_c]\n",
        "    \n",
        "    out = self.out_layer(out)\n",
        "    out = self.softmax(out)\n",
        "    return out, hidden_states\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_py0i6ssqJg",
        "colab_type": "text"
      },
      "source": [
        "### Extensions\n",
        "\n",
        "ByteNet was proposed before the Transformer model. We could try extensions of the proposed ByteNet by adding in multi-headed attention layer to give better positional encoding. The implementation of the attention layer is from [here](https://www.tensorflow.org/tutorials/text/transformer#multi-head_attention) but due to time constraints, a working Decoder and Encoder that uses this layer was not completely developed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzdWJroEu6i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Attention Layer { form-width: \"150px\" }\n",
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, hidden_units, num_heads):\n",
        "    super(AttentionLayer, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_units = hidden_units\n",
        "    \n",
        "    assert hidden_units % self.num_heads == 0\n",
        "    \n",
        "    self.depth = hidden_units // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(hidden_units)\n",
        "    self.wk = tf.keras.layers.Dense(hidden_units)\n",
        "    self.wv = tf.keras.layers.Dense(hidden_units)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(hidden_units)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, hidden_units)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, hidden_units)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, hidden_units)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.hidden_units))  # (batch_size, seq_len_q, hidden_units)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, hidden_units)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG34TwpRY19_",
        "colab_type": "text"
      },
      "source": [
        "## Training \n",
        "\n",
        "Here, we show how to train a standard ByteNet model. \n",
        "<br>\n",
        "There are two kinds of training possible - \n",
        "<br>\n",
        "The first is parallel, where the actual targets are used instead (i.e. teacher forcing), and gives us the benefit of predicting multiple characters at once and calculating loss over them (this is what the ByteNet paper uses to train). This second type of training might lead to unstable predictions at test time if the training set is small, but significantly increases training time. For example, when training on 1000 pairs of EN-FR, the sequential training takes around 7 minutes per epoch, while training using complete teacher forcing takes around 1 minute per epoch. \n",
        "<br>\n",
        "The second is sequential, where the decoder's outputs are fed back into decoder (along with the encoder's outputs). This kind of training takes a long time, and converges in a reasonable time only for datasets that are on the order of 100s of pairs. \n",
        "\n",
        "<br> \n",
        "When training, ByteNet completely learns (i.e. NLLLoss goes to 0) small datasets. For example, 100 sentence pairs take approximately 250-300 epochs to overfit, and for 1000 sentence pairs, it takes ~5000 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXgeSxUrVMf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Loss Function { form-width: \"150px\" }\n",
        "\n",
        "def loss_function(real, pred, loss_object):\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnjM7_1Phvzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train Step { form-width: \"150px\" }\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, encoder, decoder, optimizer, loss_object,\n",
        "               teacher_forcing=False):\n",
        "  \n",
        "\n",
        "  loss = 0\n",
        "  inp_len = inp.shape[1]\n",
        "  targ_len = targ.shape[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    \n",
        "    enc_output = encoder(inp)\n",
        "\n",
        "    d_inp = tf.expand_dims([targ_lang.word_index[SOS_CHAR]] * BATCH_SIZE, 1)\n",
        "    \n",
        "    for t in range(targ_len):\n",
        "      \n",
        "      # need to prepare inputs to decoder to be of appropriate dimensions\n",
        "      if t < enc_output.shape[1]:\n",
        "\n",
        "        enc_padding = tf.zeros([enc_output.shape[0], \n",
        "                                targ_len-(t+1), \n",
        "                                enc_output.shape[2]])\n",
        "        \n",
        "      else:\n",
        "        enc_padding = tf.zeros([enc_output.shape[0], \n",
        "                                targ_len-enc_out.shape[1], \n",
        "                                enc_output.shape[2]])\n",
        "      \n",
        "      e_out = tf.concat([enc_output[:,:t+1,:], enc_padding],1)\n",
        "\n",
        "      d_padding = tf.zeros([d_inp.shape[0],\n",
        "                            targ_len-(t+1)], dtype=tf.dtypes.int32)\n",
        "      \n",
        "      d_context = d_inp[:,:t+1]\n",
        "      if teacher_forcing: \n",
        "        d_context = tf.expand_dims(targ[:, t+1], 1)\n",
        "      \n",
        "      \n",
        "      d_prev = tf.concat([d_context, d_padding],1)\n",
        "      \n",
        "      # pass d_prev and e_out into decoder\n",
        "      d_out = decoder(d_prev, e_out)\n",
        "      \n",
        "      loss += loss_function(targ[:, t], d_out, loss_object)\n",
        "\n",
        "      _, i = tf.math.top_k(d_out)\n",
        "      d_inp = tf.concat([d_inp, i], axis =1) \n",
        "      \n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xwTpR_yKVss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train Step Sanity Check Set Up { form-width: \"150px\" }\n",
        "\n",
        "# use same learning rate as paper\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
        "\n",
        "# set logits to false, because we have softmax in last layer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction='none')\n",
        "\n",
        "\n",
        "# use a smaller encoder and decoder as compared to the paper for \n",
        "# this notebook \n",
        "encoder = Encoder(vocab_inp_size,hidden_units,3,2,2)\n",
        "decoder = Decoder(vocab_tar_size, hidden_units, 3, 2,2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeglF0_Ll4zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train Step Sanity Check { form-width: \"150px\" }\n",
        "loss = train_step(example_input_batch, example_target_batch, \n",
        "                  encoder, decoder, optimizer, loss_object)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3sqxchJJjFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train Step With Complete Teacher Forcing{ form-width: \"150px\" }\n",
        "\n",
        "@tf.function\n",
        "def train_step_tf(inp, targ, encoder, decoder, optimizer, loss_object,\n",
        "               teacher_forcing=False):\n",
        "  \n",
        "\n",
        "  loss = 0\n",
        "  inp_len = inp.shape[1]\n",
        "  targ_len = targ.shape[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    \n",
        "    enc_output = encoder(inp)\n",
        "    \n",
        "    d_inp = tf.expand_dims([targ_lang.word_index[SOS_CHAR]] * BATCH_SIZE, 1)\n",
        "  \n",
        "    d_context = tf.expand_dims(targ, 1)\n",
        "      \n",
        "    d_prev = tf.concat([d_inp, d_context],1)\n",
        "      \n",
        "    d_out = decoder(d_prev, e_out)\n",
        "      \n",
        "    for t in range(targ_len):\n",
        "      loss += loss_function(targ[:, t], d_out, loss_object)\n",
        "\n",
        "      \n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZbhT0CNhaxX",
        "colab_type": "text"
      },
      "source": [
        "Checkpoint saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrVA2f5ahUad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Checkpoint Saving { form-width: \"150px\" }\n",
        "\n",
        "checkpoint_dir = CKPT_DIR\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJSjW2jAarP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Training by Epoch { form-width: \"150px\" }\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "\n",
        " \n",
        "    batch_loss = train_step(inp, targ, encoder, decoder, optimizer, \n",
        "                            loss_object)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMJdxTObUvCl",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating \n",
        "\n",
        "Given the model we have trained, run it on unseen sentences. While \"greedy\" search is the easiest to implement, the paper uses beam search for evaluation. Both are shown here.\n",
        "<br>\n",
        "It is here the difficulty of using TensorFlow becomes most apparent. Several built-in methods that could make evaluation easier, such as beam search (which is implemented from scratch in this notebook for ease of presentation), require non-eager execution and other technical complications. <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNt296Z19YYB",
        "colab_type": "text"
      },
      "source": [
        "For beam search, tf.nn.ctc_beam_search_decoder requires non-eager execution, which is tricky. Going to implement a version of CTC Beam Search from scratch by following this [article](https://medium.com/the-artificial-impostor/implementing-beam-search-part-1-4f53482daabe) and this [repository](https://github.com/githubharald/CTCDecoder/blob/master/src/BeamSearch.py).<br>\n",
        "\n",
        "\n",
        "Each Node going to contain the following - <br>\n",
        "1) List of chars so far<br>\n",
        "2) Decoder output for each of the chars so far (need to calculate loss later)<br>\n",
        "3) It's own log probability<br>\n",
        "\n",
        "The basic beam search demonstrated in this example notebook follows a very basic algorithm as follows - <br>\n",
        "1) Initialize - First node contains the SOS char, and add it to prev_nodes <br>\n",
        "2) At each search iteration -<br>\n",
        " * For each node in prev_nodes, get the k most probable next chars and create k nodes for each of these chars, where k = beam_width\n",
        " * If the char is EOS, add the node to the list of candidates, else, add the node to the next_nodes list\n",
        " * Sort the next_nodes list based on the log_probs of the nodes, and keep the k top ones.\n",
        " * The next_nodes are now the new prev_nodes for the next iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TX3FxQ2JktC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Evaluate { form-width: \"150px\" }\n",
        "\n",
        "\n",
        "\n",
        "# similar to training, just add a output array and stop if EOS is reached\n",
        "def evaluate(inputs,targs, encoder, decoder, loss_object):\n",
        "\n",
        "  loss = 0\n",
        "  # this will be the MAXIMUM length of the DataSet Object - this \n",
        "  # is to prevent runaway translations\n",
        "  targ_len = targs.shape[1]\n",
        "  enc_output = encoder(inputs)\n",
        "  d_inp = tf.expand_dims([targ_lang.word_index[SOS_CHAR]] , 1)\n",
        "  output = []\n",
        "  for t in range(targ_len):\n",
        "\n",
        "    if t < enc_output.shape[1]:\n",
        "\n",
        "        enc_padding = tf.zeros([enc_output.shape[0], \n",
        "                                targ_len-(t+1), \n",
        "                                enc_output.shape[2]])\n",
        "        \n",
        "    else:\n",
        "      enc_padding = tf.zeros([enc_output.shape[0], \n",
        "                              targ_len-enc_out.shape[1], \n",
        "                              enc_output.shape[2]])\n",
        "    \n",
        "    e_out = tf.concat([enc_output[:,:t+1,:], enc_padding],1)\n",
        "\n",
        "    d_padding = tf.zeros([d_inp.shape[0],\n",
        "                          targ_len-(t+1)], dtype=tf.dtypes.int32)\n",
        "    \n",
        "    d_context = d_inp[:,:t+1]\n",
        "    \n",
        "    d_prev = tf.concat([d_context, d_padding],1) \n",
        "\n",
        "    d_out = decoder(d_prev, e_out)\n",
        "\n",
        "    loss += loss_function(targs[:, t], d_out, loss_object)\n",
        "\n",
        "    _, i = tf.math.top_k(d_out)\n",
        "    idx = tf.reduce_sum(i).numpy()\n",
        "    c = targ_lang.index_word[idx]\n",
        "    output.append(c)\n",
        "    if c==EOS_CHAR:\n",
        "      break\n",
        "    d_inp = tf.concat([d_inp, i], axis =1) \n",
        "\n",
        "  sentence_loss = loss / len(output)\n",
        "  return output, sentence_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGliSVz_U4-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Node Class { form-width: \"150px\"}\n",
        "\n",
        "\n",
        "class Node:\n",
        "  def __init__(self, decoded_chars, decoder_output, log_prob):\n",
        "    # chars already decoded in the path leading up to it \n",
        "    # basically the record of all previous states\n",
        "    # stored as indices\n",
        "    self.decoded_chars = decoded_chars\n",
        "    # output from decoder if the decoded_chars \n",
        "    # was fed into the decoder. Need this to \n",
        "    # calculate the loss \n",
        "    self.decoder_output = decoder_output\n",
        "    # log of probability, this is what we will \n",
        "    # sort on \n",
        "    self.log_prob = log_prob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZaweegZgaRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Beam Search Params { form-width: \"150px\"}\n",
        "\n",
        "beam_search_params = {\n",
        "    # maximum candidates being considered\n",
        "    'max_candidates' : 10,\n",
        "    # to prevent runaway search\n",
        "    'max_iterations' : 1000,\n",
        "    # width of the beam search - paper uses 12\n",
        "    'width' : 5\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh8CD6EYuXbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Evaluate with Beam { form-width: \"150px\"}\n",
        "\n",
        "def beam_evaluate(inputs,targs, encoder, decoder, loss_object, beam_search_params):\n",
        "\n",
        "  loss = 0\n",
        "  enc_output = encoder(inputs)\n",
        "  d_inp = tf.expand_dims([targ_lang.word_index[SOS_CHAR]] , 1)\n",
        "  output = []\n",
        "\n",
        "\n",
        "  # beam search initialize arrays \n",
        "  prev_nodes = []\n",
        "  next_nodes = []\n",
        "  candidates = []\n",
        "  beam_width = beam_search_params['width']\n",
        "\n",
        "  # make the first node \n",
        "  d_prev = d_inp[:,:1]\n",
        "  e_out = enc_output[:,:1, :]\n",
        "  d_out = decoder(d_prev, e_out)\n",
        "  initial_node = Node(d_inp, d_out, 0)\n",
        "\n",
        "\n",
        "  # put it in the prev_nodes \n",
        "  prev_nodes.append(initial_node)\n",
        "\n",
        "  # limit the number of iterations \n",
        "  # to prevent run away search\n",
        "  t = 0\n",
        "  while t <= beam_search_params['max_iterations'] and \\\n",
        "  prev_nodes and len(candidates) <= beam_search_params['max_candidates']:\n",
        "    t+=1 \n",
        "    for prev_node in prev_nodes:\n",
        "      # consider a previous node, get possible \n",
        "      # next chars\n",
        "      decoded_prev = prev_node.decoded_chars\n",
        "      l = d_prev.shape[1]\n",
        "      e_out = enc_output[:,:l,:]\n",
        "      d_out = decoder(decoded_prev, e_out)\n",
        "\n",
        "      probs, indexes = tf.math.top_k(d_out, k=beam_width)\n",
        "\n",
        "      # add all to the next nodes \n",
        "      for j in range(beam_width):\n",
        "        idx_j = tf.expand_dims(indexes[:,j], axis=1)\n",
        "        p_j = tf.expand_dims(probs[:,j], axis=1) \n",
        "        log_prob_j = np.log(p_j).flatten()[0] \n",
        "        decoded_next = tf.concat([decoded_prev, idx_j], axis= 1)\n",
        "        output_next = tf.concat([prev_node.decoder_output, d_out], axis=0)\n",
        "\n",
        "        next_node = Node(decoded_next, output_next, \n",
        "                        prev_node.log_prob + log_prob_j)\n",
        "        \n",
        "        idx_j = tf.reduce_sum(idx_j).numpy()\n",
        "        c = targ_lang.index_word[idx_j]\n",
        "\n",
        "        if c == EOS_CHAR:\n",
        "          candidates.append(next_node)\n",
        "        else:\n",
        "          next_nodes.append(next_node)\n",
        "    \n",
        "    # done adding considering new nodes for this \n",
        "    # iteration. Now, sort next_nodes \n",
        "    # and rebuild prev_nodes to keep it to beam_width\n",
        "    prev_nodes = []\n",
        "    next_nodes.sort(key = lambda x: x.log_prob, reverse=True)\n",
        "\n",
        "    # rebuild prev_nodes\n",
        "    # seem to \n",
        "    for i in range(min(beam_width, len(next_nodes))):\n",
        "      prev_nodes.append(next_nodes[i])\n",
        "    \n",
        "    # empty next nodes\n",
        "    next_nodes = []\n",
        "\n",
        "    if t+1 > beam_search_params['max_iterations'] and not candidates:\n",
        "      candidates = prev_nodes\n",
        "\n",
        "  # done with beam search - found our candidates\n",
        "  # sort them, get the top one and calculate loss \n",
        "  candidates.sort(key = lambda x: x.log_prob, reverse=True)\n",
        "  o_node = candidates[0]\n",
        "\n",
        "  output = []\n",
        "  for i in range(o_node.decoded_chars.shape[1]):  \n",
        "    idx = tf.reduce_sum(o_node.decoded_chars[0][i]).numpy()\n",
        "    output.append(targ_lang.index_word[idx])\n",
        "\n",
        "  t_len = targs.shape[1]\n",
        "  for i in range(len(output)):\n",
        "    \n",
        "    # target might be smaller than predicted, so just put \n",
        "    # EOS token\n",
        "    if i<t_len:\n",
        "      real = targs[:,i]\n",
        "    else:\n",
        "      real = tf.convert_to_tensor([targ_lang.word_index[EOS_CHAR]])\n",
        "    pred = o_node.decoder_output[i,:]\n",
        "    pred = tf.expand_dims(pred,0)\n",
        "    loss += loss_function(real, pred, loss_object)\n",
        "\n",
        "  sentence_loss = loss / len(output)\n",
        "  return output, sentence_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B6Z9wHVwnPh",
        "colab_type": "text"
      },
      "source": [
        "If the Dataset object had been split into training-validation-test sets, the preprocessing steps in the \"Evaluate Sanity Check Set Up\" would be unnecessary. But because this example notebook is for illustration purposes, all of the dataset is used for training, and we evaluate the model on single sentences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6dCJc1Lsfvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Evaluate Sanity Check Set Up { form-width: \"150px\" }\n",
        "sentence = \"we try\"\n",
        "target_sentence = \"on essaye\"\n",
        "\n",
        "sentence = preprocess_sentence(sentence, True)\n",
        "inputs = [inp_lang.word_index[c] for c in sentence]\n",
        "inputs = tf.convert_to_tensor(inputs)\n",
        "inputs = tf.expand_dims(inputs, 0)\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                       maxlen=max_length_inp, \n",
        "                                                       padding='post')\n",
        "\n",
        "target_sentence = preprocess_sentence(target_sentence)\n",
        "targs = [targ_lang.word_index[c] for c in target_sentence]\n",
        "targs = tf.expand_dims(targs, 0)\n",
        "targs = tf.keras.preprocessing.sequence.pad_sequences(targs, \n",
        "                                                      maxlen=max_length_targ, \n",
        "                                                      padding='post')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzyd6fBmmhCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Evaluate Unit Test { form-width: \"150px\" }\n",
        "\n",
        "output, sentence_loss = evaluate(inputs ,targs, encoder, \n",
        "                                 decoder, loss_object)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}