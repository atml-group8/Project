{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementation A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09LtQTmuocYe",
        "colab_type": "text"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YPVAfH3oXI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Imports { form-width: \"200px\" }\n",
        "\n",
        "# from __future__ import unicode_literals, print_function, division\n",
        "\n",
        "\n",
        "from io import open\n",
        "from itertools import islice\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import string\n",
        "import random\n",
        "import logging\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s %(levelname)s: %(message)s',\n",
        "    datefmt='%Y-%m-%d %I:%M:%S',\n",
        "    level=logging.INFO)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOuirL4fq6Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Paths { form-width: \"200px\" }\n",
        "\n",
        "GDRIVE_PATH = \"/content/drive\"\n",
        "WORK_DIR = os.path.join(\n",
        "    GDRIVE_PATH, \"path/to/project/directory/in/gdrive\")\n",
        "DATA_DIR = os.path.join(WORK_DIR, \"data\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdvTSxs8obtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Language { form-width: \"200px\" }\n",
        "\n",
        "# Start of sentence marker.\n",
        "SOS_token = 0\n",
        "SOS_char = '^'\n",
        "\n",
        "# End of sentence marker.\n",
        "EOS_token = 1\n",
        "EOS_char = '$'\n",
        "\n",
        "# Padding character for dynamic unfolding.\n",
        "PAD_token = 2\n",
        "PAD_char = '@'\n",
        "\n",
        "\n",
        "# Describes a language. The avialable characters in the language are gathered\n",
        "# and indexed by this class.\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {PAD_char: PAD_token}\n",
        "        self.char2count = {}\n",
        "        self.index2char = {\n",
        "            SOS_token: SOS_char,\n",
        "            EOS_token: EOS_char,\n",
        "            PAD_token: PAD_char,\n",
        "        }\n",
        "        # The counts includes the special characters.\n",
        "        self.n_chars = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for char in sentence:\n",
        "            self.addChar(char)\n",
        "\n",
        "    def addChar(self, char):\n",
        "        if char not in self.char2index:\n",
        "            self.char2index[char] = self.n_chars\n",
        "            self.char2count[char] = 1\n",
        "            self.index2char[self.n_chars] = char\n",
        "            self.n_chars += 1\n",
        "        else:\n",
        "            self.char2count[char] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqC_2bawpajY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Language Encoding Functions { form-width: \"200px\" }\n",
        "\n",
        "# Converts a sentence to a vector of indices matching its characters.\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.char2index[char] for char in sentence]\n",
        "\n",
        "# Converts a sentence to a tensor of indices matching its characters.\n",
        "# Depending on use-case, the indices can be tpyed as integers or floats.\n",
        "def tensorFromSentence(lang, sentence, is_float=False):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    dtype = torch.float if is_float else torch.long\n",
        "    return torch.tensor(indexes, dtype=dtype, device=device).view(-1, 1)\n",
        "\n",
        "# Converts a pair of sentences (the first in the input language, the second in\n",
        "# the output language) to a pair of tensors of indices matching their\n",
        "# characters.\n",
        "def tensorsFromPair(experiment, pair):\n",
        "    input_tensor = tensorFromSentence(experiment.input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(experiment.output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "# Converts a vector of character indices to a sentence.\n",
        "def senteceFromTensor(lang, tensor):\n",
        "    sentence = [lang.index2char[i.item()] for i in tensor]\n",
        "    return ''.join(sentence).strip(SOS_char + EOS_char)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJAx9efxptw6",
        "colab_type": "text"
      },
      "source": [
        "# Input Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJOn6yDFpucw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Parsing { form-width: \"200px\" }\n",
        "\n",
        "# Turns a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_7XvXWpqM0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Filtering { form-width: \"200px\" }\n",
        "\n",
        "BAD_CHARS = set([SOS_char, EOS_char, PAD_char, '#', '&'])\n",
        "\n",
        "def noSpecialChars(string):\n",
        "    chars = set(string)\n",
        "    return not (chars & BAD_CHARS)\n",
        "\n",
        "def filterPair(piar):\n",
        "    return noSpecialChars(piar[0]) and noSpecialChars(piar[1])\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9w-Q0YFqjfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Read Tatoeba Data { form-width: \"200px\" }\n",
        "\n",
        "# Reads the en-fr data used in the tutorial.\n",
        "def readTatoebaLangs(num=0):\n",
        "    logging.info(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open(os.path.join(DATA_DIR, \"Tatoeba/eng-fra.txt\"),\n",
        "                 encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    if num:\n",
        "        # We don't want to take the first num pairs, because the data might be\n",
        "        # somewhat sorted. But we do want experiments to be reproducible.\n",
        "        # So we use a constant seed.\n",
        "        random.seed(1234)\n",
        "        pairs = random.sample(pairs, num)\n",
        "\n",
        "    input_lang = Lang(\"en\")\n",
        "    output_lang = Lang(\"fr\")\n",
        "\n",
        "    return input_lang, output_lang, pairs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwoG7HxKrnXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Read WMT Data { form-width: \"200px\" }\n",
        "\n",
        "def readWMTLangs(num=0):\n",
        "    logging.info(\"Reading lines...\")\n",
        "\n",
        "    # Read the files and split into lines\n",
        "    l1_lines, l2_lines = [], []\n",
        "    # The first lines in WMT are messed up.\n",
        "    start = 1000\n",
        "    getLines = lambda lines: [normalizeString(l.strip()) for l in lines]\n",
        "    with open(os.path.join(DATA_DIR, \"WMT/train.en.txt\"),\n",
        "              encoding='utf-8') as fd:\n",
        "        l1_lines = getLines(islice(fd, start, start+num))\n",
        "    with open(os.path.join(DATA_DIR, \"WMT/train.de.txt\"), \n",
        "              encoding='utf-8') as fd:\n",
        "        l2_lines = getLines(islice(fd, start, start+num))\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = list(zip(l1_lines, l2_lines))\n",
        "\n",
        "    input_lang = Lang(\"en\")\n",
        "    output_lang = Lang(\"de\")\n",
        "\n",
        "    return input_lang, output_lang, pairs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lVC9-PXsl6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Prepare Data { form-width: \"200px\" }\n",
        "\n",
        "def prepareData(use_WMT, num=0):\n",
        "    prep_data_func = readWMTLangs if use_WMT else readTatoebaLangs\n",
        "    input_lang, output_lang, pairs = prep_data_func(num)\n",
        "\n",
        "    logging.info(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    logging.info(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    logging.info(\"Counting chars...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    logging.info(\"Counted chars:\")\n",
        "    logging.info(\"%s %s\", input_lang.name, input_lang.n_chars)\n",
        "    logging.info(\"%s %s\", output_lang.name, output_lang.n_chars)\n",
        "    logging.info(random.choice(pairs))\n",
        "    return input_lang, output_lang, pairs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwMFGzaftjI8",
        "colab_type": "text"
      },
      "source": [
        "# ByteNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plwm-Lcz0GBI",
        "colab_type": "text"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxE_59CY0IIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Layer Norm { form-width: \"200px\" }\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        # Takes an input of the shape (batch, channels, seq_len) and performs\n",
        "        # LayerNorm across the channels per datapoint.\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.layernorm = nn.LayerNorm(channels)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        # The PyTorch Layer Norm only works on the last dimensions, so the\n",
        "        # cannels must be moved to the end before normalisation, and then moved\n",
        "        # back.\n",
        "        out = inputs.transpose(1,2)\n",
        "        out = self.layernorm(out)\n",
        "        out = out.transpose(1,2)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AWzU9gMthrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Residual Block { form-width: \"200px\" }\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, d, k, dialation, masked=False):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.d = d # Embedding size\n",
        "        self.k = k # Kernel size\n",
        "        self.dialation = dialation\n",
        "        self.masked = masked\n",
        "\n",
        "        self.lnorm1 = LayerNorm(2*d)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv_2d_to_d = nn.Conv1d(2*d, d, kernel_size=1)\n",
        "        self.lnorm2 = LayerNorm(d)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pad = nn.ConstantPad1d(self.padding(), 0.)\n",
        "        self.conv = nn.Conv1d(d, d, k, dilation=dialation)\n",
        "        self.lnorm3 = LayerNorm(d)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.conv_d_to_2d = nn.Conv1d(d, 2*d, kernel_size=1)\n",
        "    \n",
        "    def padding(self):\n",
        "        # We use padding to preserve the length - the length of the output to\n",
        "        # the convolutional layer should be the same as that of the input.\n",
        "        p = self.dialation*(self.k-1)\n",
        "        if self.masked:\n",
        "            # For a masked convolution (i.e. a causual convolution) all the\n",
        "            # padding must be placed on the left.\n",
        "            return (p, 0)\n",
        "        return (p // 2 + p % 2, p // 2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        out = inputs\n",
        "        out = self.lnorm1(out)\n",
        "        out = self.relu1(out)\n",
        "        # We use d kernels of size 1x2d, to reduce the number of channels from\n",
        "        # 2d to d.\n",
        "        out = self.conv_2d_to_d(out)\n",
        "        out = self.lnorm2(out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        logging.debug(\"before main conv: %s\" % (out.size(),))\n",
        "        out = self.pad(out)\n",
        "        out = self.conv(out)\n",
        "        logging.debug(\"after main conv: %s\" % (out.size(),))\n",
        "\n",
        "        out = self.lnorm3(out)\n",
        "        out = self.relu3(out)\n",
        "        # We use 2d kernels of size 1xd, to expand the number of channels from d\n",
        "        # to 2d.\n",
        "        out= self.conv_d_to_2d(out)\n",
        "        # The residual connection.\n",
        "        out += inputs\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn_STBRPtoxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Residual Series { form-width: \"200px\" }\n",
        "\n",
        "# A series of residual blocks, with dialtion increasing from block to\n",
        "# block by a factor of 2.\n",
        "# Example: if num_blocks=3 we will have dilations 1, 2, 4.\n",
        "class ResidualSeries(nn.Module):\n",
        "    def __init__(self, d, k, num_blocks, masked=False):\n",
        "        super(ResidualSeries, self).__init__()\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.num_blocks = num_blocks\n",
        "        self.masked = masked\n",
        "\n",
        "        blocks = []\n",
        "        for i in range(num_blocks):\n",
        "            dialation = 1 << i\n",
        "            blocks.append(ResidualBlock(d, k, dialation, masked))\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        out = inputs\n",
        "        out = self.blocks(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G602RgfB7h8Z",
        "colab_type": "text"
      },
      "source": [
        "## CNN Variant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC36K1UytseS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Encoder { form-width: \"200px\" }\n",
        "\n",
        "class EncoderByteNet(nn.Module):\n",
        "    def __init__(self, params, input_lang):\n",
        "        super(EncoderByteNet, self).__init__()\n",
        "        logging.info(\"--Starting CNN encoder--\")\n",
        "        self.d = params.d # Embedding dimension\n",
        "        self.k = params.k # Kernel size\n",
        "        # Number of residual blocks in a series\n",
        "        self.num_blocks = params.num_blocks\n",
        "        # Number of series of residual blocks\n",
        "        self.num_series = params.num_series\n",
        "         # The rate at which the input is padded for dynamic unfolding.\n",
        "        self.unfold_rate = params.unfold_rate\n",
        "        self.input_lang = input_lang\n",
        "\n",
        "        self.conv_in = nn.Conv1d(1, 2*self.d, 1)\n",
        "        blocks = [ResidualSeries(self.d, self.k, self.num_blocks)\\\n",
        "                  for i in range(self.num_series)]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "        self.conv_out = nn.Conv1d(2*self.d, self.d, 1)\n",
        "        # Note: According to the paper, the are additional layers here:\n",
        "        # a ReLU, another convolution and a softmax.\n",
        "        # Since no information is given about the convolution (like kernel size\n",
        "        # or dilation rate), I decided not to add it. Also, a softmax seems\n",
        "        # somewhat illogical here, which makes me believe the additional layers\n",
        "        # specifed might be relevant only for the decoder). Adding just the ReLU\n",
        "        # resulted in a much less successful model*. Hence, these layers were\n",
        "        # skipped.\n",
        "        # * When training with teacher forcing, the ReLU resulted in the encoder\n",
        "        #   outputs being zeros. Without teacher forcing, the convergence rate\n",
        "        #   was simply quicker without the ReLU.\n",
        "\n",
        "    def unfold(self, input):\n",
        "        length = input.size(0)\n",
        "        pad_len = math.floor((self.unfold_rate - 1.0) * length)\n",
        "        padding = PAD_char * pad_len\n",
        "        return torch.cat(\n",
        "            (input.type(torch.float),\n",
        "             tensorFromSentence(self.input_lang, padding, True)),\n",
        "             0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        logging.debug(\"--Encoding--\")\n",
        "        logging.debug(\"input: %s\" % (input.size(),))\n",
        "\n",
        "        out = input\n",
        "        logging.debug(\"out before unfold: %s\" % (out.size(),))\n",
        "        out = self.unfold(out).permute(1,0).unsqueeze(0)\n",
        "        logging.debug(\"out before conv: %s\" % (out.size(),))\n",
        "\n",
        "        # The residual block expects the number of incoming channels to be\n",
        "        # 2d, so we use 2d filters of size 1x1 to extend the number of\n",
        "        # channels to 2d.\n",
        "        out = self.conv_in(out)\n",
        "        logging.debug(\"initial out: %s\" % (out.size(),))\n",
        "\n",
        "        # Goes through a series of residual blocks (only 4 atm).\n",
        "        out = self.blocks(out)\n",
        "\n",
        "        # Reduces the number of channels to d.\n",
        "        out = self.conv_out(out)\n",
        "        logging.debug(\"final: %s\" % (out.size(),))\n",
        "        logging.debug(\"--Done Encoding--\")\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3nWok5guCh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Decoder { form-width: \"200px\" }\n",
        "\n",
        "class DecoderByteNet(nn.Module):\n",
        "    def __init__(self, params, output_lang):\n",
        "        super(DecoderByteNet, self).__init__()\n",
        "        logging.info(\"--Starting CNN decoder--\")\n",
        "        self.d = params.d # Embedding dimension\n",
        "        self.k = params.k # Kernel size\n",
        "        # Number of residual blocks in a series\n",
        "        self.num_blocks = params.num_blocks\n",
        "        # Number of series of residual blocks\n",
        "        self.num_series = params.num_series\n",
        "\n",
        "        self.embedding = nn.Embedding(output_lang.n_chars, self.d)\n",
        "\n",
        "        blocks = [ResidualSeries(self.d, self.k, self.num_blocks, masked=True)\\\n",
        "                  for i in range(self.num_series)]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        # After the residual blocks, the number of channels is 2d. If m is the\n",
        "        # number of characters in the output language, the number of channels\n",
        "        # should be changed to m, so that a softmax can give us the probability\n",
        "        # of each of them being the next character.\n",
        "        self.conv_out = nn.Conv1d(2*self.d, output_lang.n_chars, 1)\n",
        "        # Note: According to the paper, the are additional layers before the,\n",
        "        # softmax, but they were skipped. See reasoning in the documentation of \n",
        "        # EncoderByteNet, where they were also skipped for similar reasons.\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs, context):\n",
        "        # Inputs: the previously decoded characters.\n",
        "        # Context: the respective outputs from the encoder.\n",
        "        logging.debug(\"--Decoding--\")\n",
        "        logging.debug(\"inputs: %s\" % (inputs.size(),))\n",
        "        logging.debug(\"context: %s\" % (context.size(),))\n",
        "\n",
        "        # If we are still decoding but there are no more outputs from the\n",
        "        # encoder, pad accordingly on the right.\n",
        "        if context.size(2) < inputs.size(0):\n",
        "            padding = inputs.size(0) - context.size(2)\n",
        "            context = torch.cat(\n",
        "                (context, torch.zeros(1, self.d, padding, device=device)), 2)\n",
        "            logging.debug(\"padded context: %s\" % (context.size(),))\n",
        "\n",
        "        # Gets the embeddings of the current inputs.\n",
        "        embedding_tensor = self.embedding(inputs).transpose(0,1).unsqueeze(0)\n",
        "        logging.debug(\"embedding_tensor  before   cats: %s\" %\n",
        "                        (embedding_tensor.size(),))\n",
        "        # Concatenates the outputs of the encoder to the embeddings.\n",
        "        embedding_tensor = torch.cat((embedding_tensor, context), 1)\n",
        "        logging.debug(\"embedding_tensor  with  context: %s\" %\n",
        "                        (embedding_tensor.size(),))       \n",
        "        out = embedding_tensor\n",
        "        \n",
        "        out = self.blocks(out)\n",
        "        out = self.conv_out(out)\n",
        "        logging.debug(\"after conv_out: %s\" % (out.size(),))\n",
        "        # Gets the probability for each character in the output language.\n",
        "        out = self.softmax(out)\n",
        "        logging.debug(\"grad after: %s\" % (out.grad_fn,))\n",
        "\n",
        "        logging.debug(\"--Done Decoding--\")\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYt_Z5hN7nRD",
        "colab_type": "text"
      },
      "source": [
        "## RNN Variant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA9R3cXfdjgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Recurrent ByteNet Encoder { form-width: \"200px\" }\n",
        "\n",
        "class EncoderRNNByteNet(nn.Module):\n",
        "    def __init__(self, params, input_lang):\n",
        "        super(EncoderRNNByteNet, self).__init__()\n",
        "        logging.info(\"--Starting RNN encoder--\")\n",
        "        self.d = params.d # Embedding dimension\n",
        "        self.lstm_layers = params.lstm_layers\n",
        "        self.unfold_rate = params.unfold_rate\n",
        "        self.input_lang = input_lang\n",
        "\n",
        "        self.embedding = nn.Embedding(input_lang.n_chars, self.d)\n",
        "        self.lstm = nn.LSTM(self.d, self.d, self.lstm_layers,\n",
        "                            dropout=params.dropout, bidirectional=True)\n",
        "        self.lin_out = nn.Linear(2*self.d, self.d)\n",
        "    \n",
        "    def unfold(self, inputs):\n",
        "        length = inputs.size(0)\n",
        "        pad_len = math.floor((self.unfold_rate - 1.0) * length)\n",
        "        padding = PAD_char * pad_len\n",
        "        return torch.cat(\n",
        "            (inputs,\n",
        "             tensorFromSentence(self.input_lang, padding, False)),\n",
        "             0)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        logging.debug(\"--Encoding--\")\n",
        "        logging.debug(\"input: %s\" % (inputs.size(),))\n",
        "    \n",
        "        # Gets the embeddings of the current input.\n",
        "        out = inputs\n",
        "        logging.debug(\"out before unfold: %s\" % (out.size(),))\n",
        "        out = self.unfold(out)\n",
        "        logging.debug(\"out before embedding: %s\" % (out.size(),))\n",
        "        out = self.embedding(out)\n",
        "\n",
        "        logging.debug(\"before LSTM: %s\" % (out.size(),))\n",
        "        out, hidden = self.lstm(out)\n",
        "        logging.debug(\"after LSTM: %s\" % (out.size(),))\n",
        "        logging.debug(\"hidden: %s\" % (hidden[0].size(),))\n",
        "        logging.debug(\"cell: %s\" % (hidden[1].size(),))\n",
        "        out = self.lin_out(out)\n",
        "        logging.debug(\"after lin_out: %s\" % (out.size(),))\n",
        "        out = out.transpose(0,1).transpose(1,2)\n",
        "        logging.debug(\"after transpose: %s\" % (out.size(),))\n",
        "\n",
        "        logging.debug(\"--Done Encoding--\")\n",
        "        return out, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9anp_3I58Acy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Recurrent ByteNet Decoder { form-width: \"200px\" }\n",
        "\n",
        "class DecoderRNNByteNet(nn.Module):\n",
        "    def __init__(self, params, output_lang):\n",
        "        super(DecoderRNNByteNet, self).__init__()\n",
        "        logging.info(\"--Starting RNN decoder--\")\n",
        "        self.d = params.d # Embedding dimension\n",
        "        # If an RNN encoder uses l LSTM layers, which are bidirectional, then it\n",
        "        # produces 2l hidden states. These need to be fed to the decoder's LSTM\n",
        "        # layers, hence it has 2l layers.\n",
        "        self.lstm_layers = 2*params.lstm_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_lang.n_chars, self.d)\n",
        "        self.lstm = nn.LSTM(2*self.d, self.d, self.lstm_layers,\n",
        "                            dropout=params.dropout)\n",
        "        self.lin_out = nn.Linear(self.d, output_lang.n_chars)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, input, encoder_context, decoder_context=None):\n",
        "        logging.debug(\"--Decoding--\")\n",
        "        logging.debug(\"input: %s\" % (input.size(),))\n",
        "        logging.debug(\"encoder_context: %s\" % (encoder_context.size(),))\n",
        "        if decoder_context:\n",
        "            logging.debug(\"hidden: %s\" % (decoder_context[0].size(),))\n",
        "            logging.debug(\"cell: %s\" % (decoder_context[1].size(),))\n",
        "\n",
        "        # If we are still decoding but there are no more outputs from the\n",
        "        # encoder, pad accordingly on the right.\n",
        "        if encoder_context.size(2) == 0:\n",
        "            encoder_context = torch.zeros(1, self.d, 1, device=device)\n",
        "            logging.debug(\"padded encdoer_context: %s\" %\n",
        "                          (encoder_context.size(),))\n",
        "        # Gets the embeddings of the current input.\n",
        "        embedding_tensor = self.embedding(input).unsqueeze(2)\n",
        "        logging.debug(\"embedding_tensor before cats: %s\" %\n",
        "                        (embedding_tensor.size(),))\n",
        "        # Concatenates the outputs of the encoder to the embeddings.\n",
        "        embedding_tensor = torch.cat((embedding_tensor, encoder_context), 1)\n",
        "        logging.debug(\"after concatenation, embedding_tensor size = %s\" %\n",
        "                      (embedding_tensor.size(),))\n",
        "        \n",
        "        # LSTM expects the dimenstions (seq_len, batch, input_size).\n",
        "        out = embedding_tensor.transpose(1,2)\n",
        "        logging.debug(\"before LSTM: %s\" % (out.size(),))\n",
        "        out, hidden = self.lstm(out, decoder_context)\n",
        "        logging.debug(\"after LSTM: %s\" % (out.size(),))       \n",
        "\n",
        "        out = self.lin_out(out)\n",
        "        logging.debug(\"after lin_out: %s\" % (out.size(),))\n",
        "        # softmax expects (batch, input_size).\n",
        "        out = out.transpose(1,2).squeeze(2)\n",
        "        logging.debug(\"before softmax: %s\" % (out.size(),))\n",
        "        # Gets the probability for each character in the output language.\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        logging.debug(\"--Done Decoding--\")\n",
        "        return out , hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krBOnR3_8Hx8",
        "colab_type": "text"
      },
      "source": [
        "## Infrastructure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCdL18Enq7km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Wrapper Functions { form-width: \"200px\" }\n",
        "\n",
        "# These are meant to wrap operations used in training and testing, so that all\n",
        "# variants of ByteNet may be used by the same function.\n",
        "\n",
        "def getEncoderOutput(experiment, input_tensor):\n",
        "    if experiment.params.is_rnn_enc: return experiment.encoder(input_tensor)\n",
        "    return experiment.encoder(input_tensor), None\n",
        "\n",
        "\n",
        "def getDecoderOutput(experiment, inputs, context, hidden, parallel=False):\n",
        "    output, hidden = None, None\n",
        "    if experiment.params.is_rnn_dec:\n",
        "        output, hidden = experiment.decoder(inputs, context, hidden)\n",
        "    else:\n",
        "        output = experiment.decoder(inputs, context)\n",
        "    if not parallel and not experiment.params.is_rnn_dec:\n",
        "        # Parallel means the decoder predicts all the characters in parallel,\n",
        "        # hence all outputs are needed (possible only for a CNN decoder).\n",
        "        # Otherwise prediction is char-by-char, hence only the final output is\n",
        "        # relevant.\n",
        "        output = output[:,:,-1]\n",
        "    \n",
        "    return output, hidden\n",
        "\n",
        "\n",
        "def getDecoderContext(experiment, idx, encoder_output):\n",
        "    start_idx = idx if experiment.params.is_rnn_dec else 0\n",
        "    return encoder_output[:,:,start_idx:idx+1]\n",
        "\n",
        "\n",
        "def getDecoderInputs(experiment, idx, decoder_input):\n",
        "    start_idx = idx if experiment.params.is_rnn_dec else 0\n",
        "    return decoder_input.flatten()[start_idx:idx+1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2_x25AwB0Oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Parameters Container { form-width: \"200px\" }\n",
        "\n",
        "class ModelParameters(object):\n",
        "    def __init__(self, is_rnn_enc, is_rnn_dec, embedding_dim, kernel=3,\n",
        "                 num_blocks=5, num_series=1, lstm_layers=4, unfold_rate=1.2,\n",
        "                 dropout=0.1):\n",
        "        self.is_rnn_enc = is_rnn_enc\n",
        "        # Based on the paper, we assume that a decoder cannot be a CNN when the\n",
        "        # encoder is an RNN.\n",
        "        self.is_rnn_dec = True if is_rnn_enc else is_rnn_dec\n",
        "        self.d = embedding_dim\n",
        "\n",
        "        self.k = kernel\n",
        "        # The number of residual blocks in a series (relevant only for CNNs)\n",
        "        self.num_blocks = num_blocks\n",
        "        # The number of residual block series (relevant only for CNNs)\n",
        "        self.num_series = num_series\n",
        "        \n",
        "        # The number of LSTM layers in the encoder, the decoder will have twice\n",
        "        # as many (relevant only for RNNs).\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.unfold_rate = unfold_rate\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRd9s9NqMvO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Experiment Container { form-width: \"200px\" }\n",
        "\n",
        "class Experiment(object):\n",
        "    def __init__(self, name, data_size, use_WMT, params, criterion=nn.NLLLoss(),\n",
        "                 learning_rate=0.0003, max_len=1000, beam_width=2,\n",
        "                 beam_candidates=4):\n",
        "        self.name = name\n",
        "        # Number of source-target pairs to use.\n",
        "        self.data_size = data_size\n",
        "        # Whether to use the WMT dataset (otherwise Tatoeba is used).\n",
        "        self.use_WMT = use_WMT\n",
        "        # The parameters of the model.\n",
        "        self.params = params\n",
        "\n",
        "        self.criterion = criterion\n",
        "        self.learning_rate = learning_rate\n",
        "        # An upper bound on the length of decoder predictions. This is a\n",
        "        # safeguard against infinite sequences.\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Beam serach parameters.\n",
        "        self.beam_width = beam_width\n",
        "        self.beam_candidates = beam_candidates\n",
        "\n",
        "        self.input_lang, self.output_lang, self.pairs = self.getData()\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "    \n",
        "    def getPath(self):\n",
        "        # Returns the path to the experiment.\n",
        "        return os.path.join(WORK_DIR, \"results\", self.name)\n",
        "    \n",
        "    def getData(self):\n",
        "        # Reads the data and returns the input and output languages, along with\n",
        "        # the pairs of intput-output sequences.\n",
        "        return prepareData(self.use_WMT, self.data_size)\n",
        "    \n",
        "    def createModel(self):\n",
        "        rnn_enc, rnn_dec = self.params.is_rnn_enc, self.params.is_rnn_dec\n",
        "        encoder = EncoderRNNByteNet if rnn_enc else EncoderByteNet\n",
        "        decoder = DecoderRNNByteNet if rnn_dec else DecoderByteNet\n",
        "\n",
        "        self.encoder = encoder(self.params, self.input_lang).to(device)\n",
        "        self.decoder = decoder(self.params, self.output_lang).to(device)\n",
        "\n",
        "    def loadModel(self):\n",
        "        path = self.getPath()\n",
        "        self.createModel()\n",
        "        self.encoder.load_state_dict(torch.load(os.path.join(path, \"encoder\"),\n",
        "                                                map_location=device))\n",
        "        self.decoder.load_state_dict(torch.load(os.path.join(path, \"decoder\"),\n",
        "                                                map_location=device))\n",
        "    \n",
        "    def saveModel(self):\n",
        "        path = self.getPath()\n",
        "        torch.save(self.encoder.state_dict(), os.path.join(path, \"encoder\"))\n",
        "        torch.save(self.decoder.state_dict(), os.path.join(path, \"decoder\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2HyirCVuk4c",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBuk40qQca4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Single Parallel Training Iteration { form-width: \"200px\" }\n",
        "\n",
        "# Parallel training is only possible for the CNN variant.\n",
        "# This training method matches the description in the paper: the entire target\n",
        "# sequence is fed to the decoder (except the last character) and the decoder\n",
        "# produces exactly <target length> predictions simultenously.\n",
        "# This training method is quicker than predicting char-by-char, however, if the\n",
        "# model fails to reach a perfect NLL of 0.0000, the model then perdorms very\n",
        "# badly even on training samples during test time.\n",
        "# This is because even a single error (which is bound to happen eventually) can\n",
        "# completely \"throw off\" the decoder, since the decoder is then fed this mistake\n",
        "# as a previous target character, and it is not equipped to handle this\n",
        "# situation.\n",
        "# This is probably a consequence of the small datasets we use. To conclude, if\n",
        "# the model is expected to reach a NLL of 0, then this method is recommended,\n",
        "# since it is considerable quicker. Otherwise, to reach a more effective model,\n",
        "# the non-parallel version is recommended.\n",
        "\n",
        "def trainParallel(experiment, input_tensor, target_tensor,\n",
        "                  encoder_optimizer, decoder_optimizer):\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_output, hidden = getEncoderOutput(experiment, input_tensor)\n",
        "    logging.debug(\"encoder_output: %s\" % (encoder_output.size(),)) \n",
        "\n",
        "    logging.debug(\"target_tensor: %s\" % (target_tensor.size(),))\n",
        "    target_with_sos = torch.cat((\n",
        "        torch.tensor([[SOS_token]], device=device), target_tensor), 0)\n",
        "    inputs = getDecoderInputs(experiment, target_length, target_with_sos)\n",
        "    logging.debug(\"inputs: %s\" % (inputs.size(),))\n",
        "    \n",
        "    decoder_output, hidden = getDecoderOutput(\n",
        "            experiment, inputs, encoder_output[:,:,:target_length], hidden,\n",
        "            True)\n",
        "    logging.debug(\"decoder_output: %s\" % (decoder_output.size(),))\n",
        "    loss = 0\n",
        "    for i in range(target_length):\n",
        "        loss += experiment.criterion(decoder_output[:,:,i], target_tensor[i])\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(experiment.encoder.parameters(), 2.0)\n",
        "    nn.utils.clip_grad_norm_(experiment.decoder.parameters(), 2.0)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hweFFaXlI5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Single Training Iteration { form-width: \"200px\" }\n",
        "\n",
        "# Controls how often we use real target values for training.\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(experiment, input_tensor, target_tensor,\n",
        "          encoder_optimizer, decoder_optimizer):\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_output, hidden = getEncoderOutput(experiment, input_tensor)\n",
        "    logging.debug(\"encoder_output: %s\" % (encoder_output.size(),))\n",
        "    if hidden: logging.debug(\"hidden[0]: %s\" % (hidden[0].size(),))\n",
        "    if hidden: logging.debug(\"hidden[1]: %s\" % (hidden[1].size(),))\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    loss = 0\n",
        "    \n",
        "    for i in range(target_length):\n",
        "        inputs = getDecoderInputs(experiment, i, decoder_input)\n",
        "        context = getDecoderContext(experiment, i, encoder_output)\n",
        "\n",
        "        decoder_output, hidden = getDecoderOutput(\n",
        "            experiment, inputs, context, hidden)\n",
        "        logging.debug(\"decoder_output: %s\" % (decoder_output.size(),))\n",
        "        loss += experiment.criterion(decoder_output, target_tensor[i])\n",
        "\n",
        "        if teacher_forcing:\n",
        "            decoder_input = torch.cat(\n",
        "                (decoder_input, target_tensor[i].unsqueeze(0)))\n",
        "        else:\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.cat((decoder_input, topi.detach()))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(experiment.encoder.parameters(), 2.0)\n",
        "    nn.utils.clip_grad_norm_(experiment.decoder.parameters(), 2.0)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpvLeZ2wvxgS",
        "colab_type": "text"
      },
      "source": [
        "## Multiple Iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxZTNlQevyvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Track Training Progress { form-width: \"200px\" }\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhtgjrGxv0HW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train by Iteration { form-width: \"200px\" }\n",
        "\n",
        "def trainIters(experiment, n_iters, print_every, pairs=[], with_dups=True,\n",
        "               parallel=False):\n",
        "    if parallel and experiment.params.is_rnn_dec:\n",
        "        logging.error(\"Parallel training is not possible for an RNN decoder.\")\n",
        "        return 0\n",
        "    train_func = trainParallel if parallel else train\n",
        "    \n",
        "    if not pairs: pairs = experiment.pairs\n",
        "    # Chooses a sampling function that allows or disallows duplicates (i.e.\n",
        "    # replacement) according to with_dups.\n",
        "    sample = random.choices if with_dups else random.sample\n",
        "\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss = 0  # Resets every print_every\n",
        "    total_loss = 0\n",
        "\n",
        "    lr = experiment.learning_rate\n",
        "    encoder_optimizer = optim.Adam(\n",
        "        experiment.encoder.parameters(), lr=lr, eps=1e-6)\n",
        "    decoder_optimizer = optim.Adam(\n",
        "        experiment.decoder.parameters(), lr=lr, eps=1e-6)\n",
        "\n",
        "    training_pairs = [tensorsFromPair(experiment, pair)\n",
        "        for pair in sample(pairs, k=n_iters)]\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train_func(experiment, input_tensor, target_tensor,\n",
        "                          encoder_optimizer, decoder_optimizer)\n",
        "        print_loss += loss\n",
        "        total_loss += loss\n",
        "        logging.debug(\"loss: %d\" % loss)\n",
        "\n",
        "        if not (iter % print_every) == 0: continue\n",
        "        print_loss_avg = print_loss / print_every\n",
        "        print_loss = 0\n",
        "        logging.info('%s (%d %d%%) %.4f',\n",
        "                     timeSince(start, iter/n_iters), iter,\n",
        "                     (iter/n_iters)*100, print_loss_avg)\n",
        "    \n",
        "    return total_loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h8s1Eu3fDVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train by Epoch { form-width: \"200px\" }\n",
        "\n",
        "def trainEpochs(experiment, n_epochs, print_every, pairs=[], parallel=False):\n",
        "    if parallel and experiment.params.is_rnn_dec:\n",
        "        logging.error(\"Parallel training is not possible for an RNN decoder.\")\n",
        "        return 0\n",
        "    \n",
        "    start = time.time()\n",
        "    num_pairs = experiment.data_size\n",
        "    iters_in_cycle = print_every*num_pairs\n",
        "\n",
        "    print_loss = 0  # Resets every print_every\n",
        "    total_loss = 0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # An epoch is equivalent to iterations where each pair is chosen exactly\n",
        "        # once (so n_iters=num_pairs and we disallow duplicates).\n",
        "        # We set print_every=num_pairs+1 to prevent printing.\n",
        "        loss = trainIters(\n",
        "            experiment, num_pairs, num_pairs+1, pairs, False, parallel)\n",
        "        print_loss += loss\n",
        "        total_loss += loss\n",
        "        \n",
        "        if not (epoch % print_every) == 0: continue\n",
        "        print_loss_avg = print_loss / iters_in_cycle\n",
        "        print_loss = 0\n",
        "        \n",
        "        logging.info('%s (%d %d%%) %.4f',\n",
        "                     timeSince(start, epoch/n_epochs), epoch,\n",
        "                     (epoch/n_epochs)*100, print_loss_avg)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLhdOeyMwHTt",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNxbi8-eqJcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Beam Node { form-width: \"200px\" }\n",
        "\n",
        "class BeamSearchNode(object):\n",
        "    def __init__(self, prev_node, decoded, outputs, log_prob, length):\n",
        "        self.prev_node = prev_node\n",
        "        # All the chars (in index form) decoded along the path to this node.\n",
        "        self.decoded = decoded \n",
        "        # The probability vectors output by the decoder along this path.\n",
        "        self.outputs = outputs\n",
        "        # The log probability of this path.\n",
        "        self.log_prob = log_prob\n",
        "        # The length of the path.\n",
        "        self.length = length\n",
        "\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    def eval(self):\n",
        "        return self.log_prob / float(self.length - 1 + self.eps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCMs1_xg8UbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Loss Measuring Function { form-width: \"200px\" }\n",
        "\n",
        "def getLoss(prediction, target, criterion):\n",
        "    loss = 0\n",
        "    trgt_length = len(target)\n",
        "    pred_length = len(prediction)\n",
        "\n",
        "    # The predicted length could be different from the true target length.\n",
        "    # We measure the error up to the maximum of the two, suing EOS padding.\n",
        "    # When the prediction is shorter, this means taking the distribution for the\n",
        "    # last character, which, by definition, means it predicted an EOS.\n",
        "    for i in range(max(trgt_length, pred_length)):\n",
        "        pred_dist = prediction[min(i, pred_length-1)]\n",
        "        trgt_char = target[min(i, trgt_length-1)]\n",
        "        logging.debug(\"pred_dist: %s\" % (pred_dist,))\n",
        "        logging.debug(\"trgt_char: %s\" % (trgt_char.item(),))\n",
        "        loss += criterion(pred_dist, trgt_char)\n",
        "    return loss / trgt_length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiyJK5YorgJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Test { form-width: \"200px\" }\n",
        "\n",
        "def test(experiment, input_tensor, target_tensor):\n",
        "    with torch.no_grad():\n",
        "        input_length = input_tensor.size(0)\n",
        "        target_length = target_tensor.size(0)\n",
        "\n",
        "        encoder_output, hidden = getEncoderOutput(experiment, input_tensor)\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        prediction = []\n",
        "        loss = 0\n",
        "\n",
        "        for i in range(experiment.max_len):\n",
        "            inputs = getDecoderInputs(experiment, i, decoder_input)\n",
        "            context = getDecoderContext(experiment, i, encoder_output)\n",
        "\n",
        "            decoder_output, hidden = getDecoderOutput(\n",
        "                experiment, inputs, context, hidden)\n",
        "            prediction.append(decoder_output)\n",
        "\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "            decoder_input = torch.cat((decoder_input, topi.detach()))\n",
        "\n",
        "        sentence = senteceFromTensor(experiment.output_lang, decoder_input)\n",
        "        return sentence, getLoss(prediction, target_tensor,\n",
        "                                 experiment.criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDr74FG4szsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Test with Beam Search { form-width: \"200px\" }\n",
        "\n",
        "def addNodes(node, decoder_output, candidates, next_nodes, beam_width):\n",
        "    log_prob, chars = decoder_output.topk(beam_width)\n",
        "    logging.debug(\"considering: %s\" % chars)\n",
        "    logging.debug(\"with log probs: %s\" % log_prob)\n",
        "    for i in range(beam_width):\n",
        "        decoded = torch.cat((node.decoded, chars[:,i]))\n",
        "        new_node = BeamSearchNode(\n",
        "            node, decoded, \n",
        "            node.outputs + [decoder_output],\n",
        "            node.log_prob+log_prob[0][i], node.length+1)\n",
        "        if chars[0,i] == EOS_token:\n",
        "            candidates.append((new_node.eval(), new_node))\n",
        "            logging.debug(\"added candidate: %s\" % decoded)\n",
        "        else:\n",
        "            next_nodes.append((new_node.eval(), new_node))\n",
        "            logging.debug(\"added next node: %s\" % decoded)\n",
        "\n",
        "def chooseNodes(next_nodes, beam_width):\n",
        "    next_nodes.sort()\n",
        "    prev_nodes = []\n",
        "    logging.debug(\"possible next nodes: %s\" % (next_nodes,))\n",
        "    for i in range(min(beam_width, len(next_nodes))):\n",
        "        logging.debug(\"promoted node %d\" % i)\n",
        "        prev_nodes.append(next_nodes[-i])\n",
        "    return prev_nodes\n",
        "\n",
        "\n",
        "def beamTest(experiment, input_tensor, target_tensor):\n",
        "    with torch.no_grad():\n",
        "        input_length = input_tensor.size(0)\n",
        "        target_length = target_tensor.size(0)\n",
        "\n",
        "        encoder_output, hidden = getEncoderOutput(experiment, input_tensor)\n",
        "        decoder_input = torch.tensor([SOS_token], device=device)\n",
        "\n",
        "        beam_width = experiment.beam_width\n",
        "        candidates_num = experiment.beam_candidates\n",
        "        candidates, prev_nodes, next_nodes = [], [], []\n",
        "        \n",
        "        initial_node = BeamSearchNode(None, decoder_input, [], 0, 1)\n",
        "        prev_nodes.append((initial_node.eval(), initial_node))\n",
        "\n",
        "        idx = 0\n",
        "        while (prev_nodes\n",
        "               and len(candidates) <= candidates_num\n",
        "               and idx < experiment.max_len):\n",
        "            logging.debug(\"have %d prev_nodes\" % len(prev_nodes))\n",
        "            for _, node in prev_nodes:\n",
        "                inputs = getDecoderInputs(experiment, idx, node.decoded)\n",
        "                context = getDecoderContext(experiment, idx, encoder_output)\n",
        "                logging.debug(\"inputs: %s\" % (inputs,))\n",
        "\n",
        "                decoder_output, hidden = getDecoderOutput(\n",
        "                    experiment, inputs, context, hidden)\n",
        "                logging.debug(\"decoder_output: %s\" % decoder_output)\n",
        "                addNodes(node, decoder_output, candidates, next_nodes,\n",
        "                         beam_width)\n",
        "            \n",
        "            prev_nodes = chooseNodes(next_nodes, beam_width)\n",
        "            next_nodes = []\n",
        "            idx += 1\n",
        "\n",
        "        if not candidates:\n",
        "            # It's possible all predictions are longer than the maximum allowed\n",
        "            # length, so no true candidates exist.\n",
        "            # In this case, we use the incomplete predictions instead.\n",
        "            logging.debug(\"no candidates found.\")\n",
        "            candidates = prev_nodes\n",
        "        \n",
        "        prediction = sorted(candidates)[-1][1]\n",
        "        sentence = senteceFromTensor(experiment.output_lang, prediction.decoded)\n",
        "        loss = getLoss(prediction.outputs, target_tensor, experiment.criterion)\n",
        "        return sentence, loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw-jbtF2U1eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Test and Evaluate Randomly { form-width: \"200px\" }\n",
        "\n",
        "def testAndEval(experiment, use_beam, num=10, print_every=0, pairs=[]):\n",
        "    if not pairs: pairs = experiment.pairs\n",
        "    if not print_every: print_every = max(1, num // 10)\n",
        "    eval_func = beamTest if use_beam else test\n",
        "    test_pairs = random.sample(pairs, num)\n",
        "    \n",
        "    total_loss = 0\n",
        "    for i, (src, dst) in enumerate(test_pairs):\n",
        "        input_tensor = tensorFromSentence(experiment.input_lang, src)\n",
        "        output_tensor = tensorFromSentence(experiment.output_lang, dst)\n",
        "        prediction, loss = eval_func(experiment, input_tensor, output_tensor)\n",
        "        total_loss += loss\n",
        "        \n",
        "        if (i % print_every) == 0:\n",
        "            print('>', src)\n",
        "            print('=', dst)\n",
        "            print('<', prediction)\n",
        "            print('Loss: %f\\n' % loss)\n",
        "    \n",
        "    print('Average loss: %f' % (total_loss / num))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cKrWzhNw6Jk",
        "colab_type": "text"
      },
      "source": [
        "# Run Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NeGCrzBxCQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Mount Google Drive { form-width: \"200px\" }\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(GDRIVE_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hgkh7a0SNlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Initialise Experiment { form-width: \"200px\" }\n",
        "\n",
        "params = ModelParameters(\n",
        "    is_rnn_enc=False,\n",
        "    is_rnn_dec=False,\n",
        "    embedding_dim=100,\n",
        "    num_series=1,\n",
        "    lstm_layers=8,\n",
        ")\n",
        "\n",
        "experiment = Experiment(\n",
        "    name=\"experiment_name\",\n",
        "    data_size=1000,\n",
        "    use_WMT=False,\n",
        "    params=params,\n",
        "    learning_rate=0.0003\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdKSwkmoxKPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train (by Iteration) { form-width: \"200px\" }\n",
        "\n",
        "experiment.createModel()\n",
        "# experiment.loadModel()\n",
        "num_sets = 4\n",
        "\n",
        "for i in range(1, num_sets+1):\n",
        "    logging.info(\"--Training set %d--\" % i)\n",
        "    trainIters(experiment, 1000, print_every=100)\n",
        "    experiment.saveModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWMEjpuO5FoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train (by Epoch) { form-width: \"200px\" }\n",
        "\n",
        "# experiment.createModel()\n",
        "experiment.loadModel()\n",
        "num_sets = 8\n",
        "\n",
        "for i in range(1, num_sets+1):\n",
        "    logging.info(\"--Training set %d--\" % i)\n",
        "    trainEpochs(experiment, 50, print_every=5, parallel=True)\n",
        "    experiment.saveModel()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6mleU9zxwJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Evaluate { form-width: \"200px\" }\n",
        "\n",
        "experiment.loadModel()\n",
        "testAndEval(experiment, False, num=1, print_every=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}