{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42226,
     "status": "ok",
     "timestamp": 1587871230872,
     "user": {
      "displayName": "Igro Nik",
      "photoUrl": "",
      "userId": "16836536084724789425"
     },
     "user_tz": -60
    },
    "id": "e7R2X2YUzNPg",
    "outputId": "7f2877a5-91d4-4390-f986-b6bf20e903e1"
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### REQUIREMENTS ###\n",
    "####################\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "from itertools import islice # For reading only a part of the data file\n",
    "from collections import OrderedDict # For defining a variable-length nn.Sequential()\n",
    "from collections import defaultdict # Used in readData()\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import math # For math.ceil() in readLine()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive/')\n",
    "#!mkdir -p /content/MyDrive\n",
    "#!mount --bind /content/gdrive/My\\ Drive /content/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABHoDZVsQ1F9"
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "### CONTROL CONSTANTS ###\n",
    "#########################\n",
    "\n",
    "# Major.\n",
    "DS_SIZE = 100000 # The number of sentences in the dataset to be fetched. After changing, set LOAD_DATASET_FROM_FILE = False.\n",
    "PATH = '.\\\\AML\\\\' # WINDOWS STYLE. The working directory with all files (above all, bytenet.train.tar & bucketised_batches_0).\n",
    "LOAD_DATASET_FROM_FILE = True # Read the training pairs either from bucketised_batches_0 (True), or from train.(en|de).\n",
    "LOAD_MODEL_FROM_FILE = False # Training only. Read the model from bytenet.train.tar (True) or start training from scratch.\n",
    "torch.backends.cudnn.enabled = True # cuDNN is NVIDIA's library of primitives, primarily for CNNs. 9x speed up.\n",
    "TEACHER_FORCING = True # Both can be used. No TEACHER_FORCING results in a massive performance hit ().\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "INSPECT_VISUALLY = False\n",
    "LIMIT_OUTPUT_PER_BATCH = 1\n",
    "\n",
    "# Minor.\n",
    "LOG = open(PATH + 'log', mode='w') # For debugging purposes. Mainly outputs sizes of tensors in intermediate computations.\n",
    "LOGGING = False # But has not been used in practice since very early stages.\n",
    "CUDA_LAUNCH_BLOCKING = 1 # Used for more meaningful GPU error messages. \n",
    "START_ITER = 0 # If not LOAD_MODEL_FROM_FILE, you can start training it from batch START_ITER. Not particularly useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-4R4p4ujHz4"
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### DATA-PREPROCESSING ###\n",
    "##########################\n",
    "\n",
    "# BATCHING\n",
    "\n",
    "PAD_TO = 50 # During training the input and target sequences will be padded to the nearest multiple of PAD_TO for efficient batching.\n",
    "\n",
    "# We will accumulate (source, target) pairs with (m * PAD_TO, n * PAD_TO) characters in the bucketised_tensor_pairs\n",
    "# dictionary with key (m,n). Whenever k pairs are accumulated in the entry corresponding to the pair (m,n) with \n",
    "# k * max(m,n) * PAD_TO >= MAX_BATCH_SIZE, the entry is converted into a batch tensor and flushed. \n",
    "# After all this steps, bucketised_tensor_pairs is likely still non-empty. All buckets satisfying the same inequality \n",
    "# with MIN_BATCH_SIZE instead are also turned into batches. All others are discarded.\n",
    "\n",
    "MIN_BATCH_SIZE = 2000\n",
    "MAX_BATCH_SIZE = 3000\n",
    "\n",
    "# VOCABULARY \n",
    "\n",
    "SOS_token = '\\2'\n",
    "EOS_token = '\\3'\n",
    "PSC_token = '\\4' # Padding Sequence Character (for Dynamic Unfolding)\n",
    "NAC_token = '\\5' # Not A Character (for batching: pad the source and target to the multiple of PAD_TO)\n",
    "UCF_token = '\\7' # Unknown Character Found (replace all unknown characters with this token)\n",
    "\n",
    "# Below, the last unicode symbols are 8 German letters with umlauts and 2 Eszetts.\n",
    "all_letters = SOS_token + EOS_token + UCF_token + PSC_token + NAC_token + string.printable + \"£€°\\u00E4\\u00EF\\u00F6\\u00FC\\u00DF\\u00C4\\u00CF\\u00D6\\u00DC\\u1E9E\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# TEXT PROCESSING\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    html_subs = {'&quot;': '\"', '&apos;': '\\'', '&amp;': '&', '&#91;': '[', '&#93;': ']', '&lt;': '<',  '&gt;': '>', '&#124;': '|'}\n",
    "\n",
    "    # NFD = Combine characters that \"have the same meaning\"\n",
    "    # unicodedata.category(c) == 'Mn' <==> Unicode Non-spacing Mark\n",
    "    for sub in html_subs:\n",
    "        s = s.replace(sub, html_subs[sub])\n",
    "    s = s.replace(' ##AT##-##AT## ', '-').replace('##STAR##','*').replace('##UNDERSCORE##','_').replace('##AT##','@').replace('\\n', '')\n",
    "\n",
    "    NFD =  [c for c in unicodedata.normalize('NFD', s)\n",
    "            if (unicodedata.category(c) != 'Mn' or c == '\\u0308')] # Leave the umlaut!!!\n",
    "\n",
    "    # But if the umlaut is not over 'a', 'o' or 'u', German doesn't have such a letter. Drop the umlaut.\n",
    "    NFD_filter_umlauts = [NFD[0]]\n",
    "    for i in range(1, len(NFD)):\n",
    "        if NFD[i] != '\\u0308' or NFD[i - 1] in 'AIOUaiou': \n",
    "            NFD_filter_umlauts.append(NFD[i])   \n",
    "\n",
    "    return unicodedata.normalize('NFC', ''.join([c if c in all_letters or c == '\\u0308'\n",
    "                    else UCF_token # Insert a UCF_token in place of c otherwise\n",
    "                    for c in NFD_filter_umlauts]))\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    assert all_letters.find(letter) != -1\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# This function can be a startling experiment. I decided to keep 2 different paddings to convey different ideas and \n",
    "# hopefully aid the model in transation. The input to the encoder does not get a SOS_token, but gets the padding \n",
    "# immediately afterwards (20% of its original length, as specified by the ByteNet paper), THEN the EOS_token (hoping that  \n",
    "# putting EOS AFTER padding will help the network to understand where approximately it should stop predicting the output), \n",
    "# and then NAC_tokens to pad the sequence to a multiple of PAD_TO.\n",
    "# Although this means that I use 2 different paddings: 1 for Dynamic Unfolding from the paper, and 1 for batching, those \n",
    "# paddings are inherently different: the 1st one means there is no information supplied, but an encoder output is expected\n",
    "# in order to help the decoder in translation, whereas the 2nd one bears no meaning: there is no new information, and\n",
    "# no output is expected.\n",
    "def readLine(line, type): \n",
    "    if type == 'enc':\n",
    "        tmp = unicodeToAscii(line.strip()) + PSC_token * int(0.2 * len(line)) + EOS_token\n",
    "    else:\n",
    "        tmp = (SOS_token if TEACHER_FORCING else '') + unicodeToAscii(line.strip()) + EOS_token\n",
    "    tmp += NAC_token * (math.ceil(len(tmp) / PAD_TO) * PAD_TO - len(tmp))\n",
    "    return tmp\n",
    "\n",
    "# Turn a line into a <BATCH_SIZE x line_length x n_letters> array of letter indices.\n",
    "def bucketToBatch(bucket):\n",
    "    source_tensor = torch.zeros(len(bucket), len(bucket[0][0]), dtype=torch.int64, device=DEVICE)\n",
    "    target_tensor = torch.zeros(len(bucket), len(bucket[0][1]), dtype=torch.int64, device=DEVICE)\n",
    "    for i in range(len(bucket)):\n",
    "        for char_pos, char in enumerate(bucket[i][0]):\n",
    "            source_tensor[i][char_pos] = letterToIndex(char)\n",
    "        for char_pos, char in enumerate(bucket[i][1]):\n",
    "            target_tensor[i][char_pos] = letterToIndex(char)\n",
    "\n",
    "    return (source_tensor, target_tensor)\n",
    "\n",
    "def readData(prefix, lang1, lang2, N = None):\n",
    "    bucketised_batches = []\n",
    "\n",
    "    lines1 = open(PATH + '%s.%s' % (prefix, lang1), encoding='utf-8')\n",
    "    lines2 = open(PATH + '%s.%s' % (prefix, lang2), encoding='utf-8')\n",
    "\n",
    "    # Read in only the first N lines of the train files. N = None means reading the entire file.\n",
    "    bucketised_tensor_pairs = defaultdict(list)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for line1, line2 in islice(zip(lines1, lines2), N):\n",
    "        i += 1\n",
    "        if DS_SIZE >= 100 and i % ((DS_SIZE // 100) * 5) == 0:\n",
    "            print(\"%d%% of the datafile was read.\" % (i / (DS_SIZE // 100)))\n",
    "        tmp1 = readLine(line1, 'enc')\n",
    "        tmp2 = readLine(line2, 'dec')\n",
    "        cur_bucket = bucketised_tensor_pairs[(len(tmp1),len(tmp2))]\n",
    "        cur_bucket.append((tmp1, tmp2)) #.append((lineToTensor(tmp1), lineToTensor(tmp2)))\n",
    "        if len(cur_bucket) * max(len(tmp1), len(tmp2)) >= MAX_BATCH_SIZE:\n",
    "            bucketised_batches.append(bucketToBatch(cur_bucket))\n",
    "            del bucketised_tensor_pairs[(len(tmp1),len(tmp2))]\n",
    "\n",
    "    for key in bucketised_tensor_pairs.keys():\n",
    "        if len(bucketised_tensor_pairs[key]) * max(key[0], key[1]) >= MIN_BATCH_SIZE:\n",
    "            bucketised_batches.append(bucketToBatch(bucketised_tensor_pairs[key]))\n",
    "\n",
    "    if bucketised_batches:\n",
    "        torch.save(bucketised_batches, PATH + 'bucketised_batches_0')\n",
    "\n",
    "if not LOAD_DATASET_FROM_FILE:\n",
    "    readData('train', 'en','de', DS_SIZE)\n",
    "bucketised_batches = torch.load(PATH + 'bucketised_batches_0', map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOqeJNfZxlBC"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### MODEL DEFINITION ###\n",
    "########################\n",
    "\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "\n",
    "# CONVOLUTION PARAMETER DEFINITIONS\n",
    "\n",
    "# wdt = width; str = stride; pad = padding; dil = dilation\n",
    "conv1x1 = {'kernel_size': 1, 'stride': 1, 'padding': 0, 'dilation': 1}\n",
    "conv1xk = {'kernel_size': 3, 'stride': 1, 'padding': 1} # Keep wdt odd and pad to (wdt+1)/2. On changing wdt MAKE SURE TO TEST masking in ResBlock::forward!!!\n",
    "\n",
    "\n",
    "\n",
    "# ENCODER & DECODER\n",
    "\n",
    "# The layer norm across the dimension of channels (we generally store inte tensors as: dim 0 -> batch; dim 1 -> channel;\n",
    "# dim 2 -> character; since this order is assumed by convolutions). However, if we calculate layer norm along the dimension\n",
    "# of characters, this will cause leakage from future characters to the previous ones (because for each channel of the former\n",
    "# LayerNorm will assign its value based on the values of all characters in this channel indiscriminately).\n",
    "# Therefore, we need a different LayerNorm that will first transpose the input before normalising them.\n",
    "class ChannelLayerNorm(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelLayerNorm, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.trueBN = nn.Sequential(\n",
    "            nn.LayerNorm(channels)\n",
    "        ) # We trick LayerNorm into thinking that channels are actually characters. Thus, the normalisation is going over the channels. \n",
    "        # A similar trick for BatchNorm itself would require knowing the number of characters ahead of time, which is feasible for compression, but infeasible for translation.\n",
    "    def forward(self, batch):\n",
    "        batch = torch.transpose(batch,1,2) # Make 1 the dimension of characters and 2 the dimension of channels.\n",
    "        return torch.transpose(self.trueBN(batch),1,2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    # [self.type == 'enc'] <=> Encoder; everything else is Decoder.\n",
    "    \n",
    "    def __init__(self, channels, dilation, type):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.type = type\n",
    "        channel_factor = 2 if type != 'enc' else 1\n",
    "        conv1xk_dil = copy.deepcopy(conv1xk)\n",
    "        conv1xk_dil['padding'] *= dilation\n",
    "        conv1xk_dil['dilation'] = dilation\n",
    "\n",
    "        # Note that the size of padding in conv1xk is chosen so that the size of the output of the block is equal to the size of the input.\n",
    "        self.resblock = nn.Sequential(OrderedDict([\n",
    "            ('norm_1', ChannelLayerNorm(channel_factor * channels)),\n",
    "            ('relu_1', nn.ReLU()),\n",
    "            ('conv1x1_1', nn.Conv1d(channel_factor * channels, channels, **conv1x1)),\n",
    "            ('norm_2', ChannelLayerNorm(channels)),\n",
    "            ('relu_2', nn.ReLU()),\n",
    "            ('conv1xd_2', nn.Conv1d(channels, channels, **conv1xk_dil)),\n",
    "            ('norm_3', ChannelLayerNorm(channels)),\n",
    "            ('relu_3', nn.ReLU()),\n",
    "            ('conv1x1_3', nn.Conv1d(channels, channel_factor * channels, **conv1x1))\n",
    "         ]))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # In the decoder we have to mask future tokens. It would make sense to mask the future target tokens (otherwise \n",
    "        # we feed into the network what we want to get from it) ONLY, but leave the encoder output unmasked (like it is \n",
    "        # done, e.g., in Transformers).\n",
    "        # If we mask both the target and the encoder output tokens, this creates a bottleneck: during testing, the output \n",
    "        # of the network is heavily dependent upon the first character it predicts (because it is used predicting all \n",
    "        # other characters). So we really want to predict it correctly. However, this character itself is predicted based\n",
    "        # on SOS_token and just 1 (as opposed to r, the size of the receptive field) encoder outputs.\n",
    "        # We probably need only 6-7 next characters to predict this 1st character, but they now all have to be encapsulated\n",
    "        # in the 1st encoder output vector of size 1 x EMBED_DIM. We thus create a bottleneck less severe, but still\n",
    "        # noticeable, than in seq2seq, where we had to encapsulated the entire sentence in such a vector.\n",
    "        \n",
    "        # The authors, however, do not provide any explanation as to how they solve this issue. The implementation would\n",
    "        # be quite untrivial: we need to prevent the leakage from target tokens' channels into the channels of the encoder \n",
    "        # outputs (because those are would be allowed to leak to previous tokens). This requires modifying LayerNorms to serve\n",
    "        # channels 0 ... d-1 and d ... 2d-1 separately, as well as convolutions (in PyTorch convolutions allow for grouping\n",
    "        # which solves this issue). Sadly, I will have to ignore this issue in my implementation: it was not found until \n",
    "        # 26th April, after reviewing the architecture from Attention Is All You Need.\n",
    "        if self.type != 'enc':\n",
    "            c_out, c_in, wdt = self.resblock._modules['conv1xd_2'].weight.data.size()\n",
    "            #self.resblock._modules['conv1xd_2'].weight.data[:,0:c_in//2,wdt//2+1:wdt] = torch.nn.Parameter(torch.zeros(c_out, c_in//2, wdt-wdt//2-1, device=DEVICE))\n",
    "            self.resblock._modules['conv1xd_2'].weight.data[:,:,wdt//2+1:wdt] = torch.nn.Parameter(torch.zeros(c_out, c_in, wdt-wdt//2-1, device=DEVICE))\n",
    "        \n",
    "        if LOGGING:\n",
    "            print('def ResBlock::forward(self, input)', file=LOG)\n",
    "            print(input.size(), file=LOG)\n",
    "            print(step1.size(), file=LOG)\n",
    "            print(step2.size(), file=LOG)\n",
    "            print(step3.size(), file=LOG, end='\\n')\n",
    "        \n",
    "        output = self.resblock(input)\n",
    "        return output + input\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, channels, res_sets, res_blocks, type):\n",
    "        super(CNN, self).__init__()\n",
    "        channel_factor = 2 if type != 'enc' else 1\n",
    "        self.type = type\n",
    "        self.embed = nn.Embedding(input_dim, channels)\n",
    "        self.channels = channels\n",
    "\n",
    "        layers = OrderedDict()\n",
    "        for res_set in range(res_sets):\n",
    "            for res_block in range(res_blocks):\n",
    "                layers['res_set_' + str(res_set) + '|res_block_' + str(res_block)] = ResBlock(channels, 2 ** res_block, type)\n",
    "\n",
    "        if type != 'enc':\n",
    "            layers['fin|conv1x1'] = nn.Conv1d(channel_factor * channels, channels, **conv1x1)\n",
    "            layers['fin|relu'] = nn.ReLU()\n",
    "            layers['fin|conv1xd'] = nn.Conv1d(channels, n_letters, **conv1xk)\n",
    "            layers['fin|logsoftmax'] = nn.LogSoftmax(dim=1)\n",
    "        self.CNN = nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, source=None, target=None, encoder_output=None):\n",
    "        # After the embedding we transpose dimensions 1 and 2 to obtain: \n",
    "        # dim = 0 is the batch, dim = 1 is the channel, dim = 2 is the character in a sequence.\n",
    "        \n",
    "        #print('In forward():')\n",
    "        if self.type == 'enc':\n",
    "            if LOGGING:\n",
    "                print('def CNN::forward(self, source, target, encoder_output) self.type == enc', file=LOG)\n",
    "                print('source', file=LOG)\n",
    "                print(source.size(), file=LOG, end='\\n')\n",
    "                #print(source, file=LOG)\n",
    "\n",
    "            emb = torch.transpose(self.embed(source), 1, 2)\n",
    "        else:\n",
    "            if LOGGING:\n",
    "                print('def CNN::forward(self, source, target, encoder_output) self.type == dec', file=LOG)\n",
    "                print('target', file=LOG)\n",
    "                print(target.size(), file=LOG)\n",
    "                #print(target, file=LOG)\n",
    "                print('encoder_output', file=LOG)\n",
    "                print(encoder_output.size(), file=LOG)\n",
    "                #print(encoder_output, file=LOG)\n",
    "            \n",
    "            tmp = torch.transpose(self.embed(target), 1, 2)\n",
    "            \n",
    "            if LOGGING:\n",
    "                print('embedding of the target', file=LOG)\n",
    "                print(tmp.size(), file=LOG, end='\\n')\n",
    "                #print(tmp)\n",
    "            \n",
    "            emb = torch.zeros(tmp.size(0), 2 * tmp.size(1), tmp.size(2), device=DEVICE)\n",
    "            out_length = min(tmp.size(2), encoder_output.size(2))\n",
    "            # The top self.channels channels will be the target sequence. The bottom self.channels ones will be the encoder output.\n",
    "            emb[:,:self.channels,:] = tmp\n",
    "            emb[:,self.channels:2*self.channels,0:out_length] = encoder_output[:,:,0:out_length]\n",
    "\n",
    "            c_out, c_in, wdt = self.CNN._modules['fin|conv1xd'].weight.data.size()\n",
    "            #self.CNN._modules['fin|conv1xd'].weight.data[:,0:c_in//2,wdt//2+1:wdt] = torch.nn.Parameter(torch.zeros(c_out, c_in//2, wdt-wdt//2-1, device=DEVICE))\n",
    "            self.CNN._modules['fin|conv1xd'].weight.data[:,:,wdt//2+1:wdt] = torch.nn.Parameter(torch.zeros(c_out, c_in, wdt-wdt//2-1, device=DEVICE))\n",
    "        return self.CNN(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2WE1IAtx2sx"
   },
   "outputs": [],
   "source": [
    "################\n",
    "### TRAINING ###\n",
    "################\n",
    "\n",
    "# TIMING [COPY-PASTED FROM THE PYTORCH TUTORIAL]\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(s / percent - s))\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "EMBED_DIM = 200\n",
    "\n",
    "# Constants from the paper.\n",
    "\n",
    "ADAM_LEARNING_RATE = 0.0003\n",
    "RES_SETS = 6        # Number of sets of residual blocks\n",
    "RES_BLOCKS = 5      # Number of residual blocks per set\n",
    "\n",
    "# Define 1 GD step. W/o Teacher Forcing\n",
    "def train_no_TF(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    target_length = target_tensor.size(1)\n",
    "\n",
    "    encoder_output = encoder(source=input_tensor)\n",
    "\n",
    "    output = torch.zeros(target_tensor.size(0), target_length, dtype=torch.int64, device=DEVICE, requires_grad=False)\n",
    "    for i in range(target_tensor.size(0)):\n",
    "        output[i][0] = letterToIndex(SOS_token)\n",
    "    loss = 0\n",
    "\n",
    "    for char_index in range(1, target_length):\n",
    "        decoder_output = decoder(target=output[:,:char_index], encoder_output=encoder_output[:,:,:char_index])\n",
    "        loss_per_char = criterion(decoder_output[:,:,char_index-1], target_tensor[:,char_index-1])\n",
    "        loss += loss_per_char.detach()\n",
    "\n",
    "        if LOGGING:\n",
    "            print('In def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)', file=LOG)\n",
    "            print('char_index = %d' % char_index, file=LOG)\n",
    "            print(output.size(), file=LOG)\n",
    "            print(decoder_output.size(), file=LOG)\n",
    "\n",
    "        loss_per_char.backward(retain_graph=True)\n",
    "        output[:,char_index] = torch.tensor(np.argmax(decoder_output[:,:,char_index-1].detach().numpy(), axis=1), device=DEVICE)\n",
    "                               \n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.detach() / target_length\n",
    "\n",
    "# Define 1 GD step. W/ Teacher Forcing\n",
    "def train_TF(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    target_length = target_tensor.size(1)\n",
    "\n",
    "    encoder_output = encoder(source=input_tensor)\n",
    "    decoder_output = decoder(target=target_tensor, encoder_output=encoder_output) # Teacher forcing\n",
    "\n",
    "    loss = criterion(decoder_output[:,:,:-1], target_tensor[:,1:])\n",
    "\n",
    "    loss.backward()                               \n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.detach() \n",
    "\n",
    "# Define the actual training.\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, save_every=10000):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    start_iter = START_ITER\n",
    "\n",
    "    # Choose whether to use teacher forcing or not.\n",
    "    train = lambda it, tt, e, d, eo, do, c: train_TF(it, tt, e, d, eo, do, c) if TEACHER_FORCING else train_no_TF(it, tt, e, d, eo, do, c)\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), ADAM_LEARNING_RATE)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), ADAM_LEARNING_RATE)\n",
    "\n",
    "    if LOAD_MODEL_FROM_FILE:\n",
    "        checkpoint = torch.load(PATH + 'bytenet.train.tar', map_location=DEVICE)\n",
    "        start_iter = checkpoint['iter']\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "        encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "        decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "\n",
    "        for param_group in encoder_optimizer.param_groups:\n",
    "            param_group['lr'] = ADAM_LEARNING_RATE\n",
    "        for param_group in decoder_optimizer.param_groups:\n",
    "            param_group['lr'] = ADAM_LEARNING_RATE\n",
    "\n",
    "        encoder.to(DEVICE)\n",
    "        decoder.to(DEVICE)\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        print('Loaded the model from %sbytenet.train.tar, which has already been trained for %d iterations.' % (PATH, start_iter))\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(start_iter + 1, start_iter + n_iters + 1):    \n",
    "        input_tensor, target_tensor = bucketised_batches[iter % len(bucketised_batches)]\n",
    "        if LOGGING:\n",
    "            print('def trainIters(encoder, decoder, n_iters, print_every, plot_every, save_every, learning_rate)', file=LOG)\n",
    "            print('iter = %d' % iter, file=LOG)\n",
    "            print(input_tensor.size(), file=LOG)\n",
    "            print(target_tensor.size(), file=LOG, end='\\n')\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            real_iter = iter - start_iter\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, real_iter / n_iters), iter, real_iter / n_iters * 100, print_loss_avg),end=(' ' if iter % save_every == 0 else '\\n'))\n",
    "\n",
    "        if iter % save_every == 0:\n",
    "            torch.save({\n",
    "                'iter': iter,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "            }, PATH + 'bytenet.train.tar')\n",
    "            print('Saved!')\n",
    "    \n",
    "encoder = CNN(n_letters, EMBED_DIM, RES_SETS, RES_BLOCKS, 'enc').to(DEVICE)\n",
    "decoder = CNN(n_letters, EMBED_DIM, RES_SETS, RES_BLOCKS, 'dec').to(DEVICE)\n",
    "\n",
    "trainIters(encoder, decoder, 1000000, print_every=300, save_every=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes Windows OS. Change to \"copy /y\" to \"cp -f\" on Linux.\n",
    "!copy /y {PATH + 'bytenet.train.tar'} {PATH + 'bytenet.eval.tar'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "### EVALUATION ###\n",
    "##################\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "# TIMING\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(s / percent - s))\n",
    "\n",
    "# EVALUATE\n",
    "\n",
    "OVER_LENGTH = 50\n",
    "\n",
    "EMBED_DIM = 200\n",
    "\n",
    "# Constants from the paper.\n",
    "\n",
    "RES_SETS = 6        # Number of sets of residual blocks\n",
    "RES_BLOCKS = 5      # Number of residual blocks per set\n",
    "\n",
    "# Define 1 evaluation step.\n",
    "def evaluate(input_tensor, target_tensor, encoder, decoder, criterion):\n",
    "    batch_size = target_tensor.size(0)\n",
    "    target_length = target_tensor.size(1)\n",
    "\n",
    "    encoder_output = encoder(source=input_tensor)\n",
    "\n",
    "    output = torch.zeros(batch_size, target_length + OVER_LENGTH, dtype=torch.int64, device=DEVICE, requires_grad=False)\n",
    "    for i in range(batch_size):\n",
    "        output[i][0] = letterToIndex(SOS_token)\n",
    "    loss = 0\n",
    "\n",
    "    for char_index in range(1, target_length + OVER_LENGTH):\n",
    "        decoder_output = decoder(target=output[:,:char_index], encoder_output=encoder_output[:,:,:char_index])\n",
    "        \n",
    "        if char_index <= target_length:\n",
    "            loss_per_char = criterion(decoder_output[:,:,char_index-1], target_tensor[:,char_index-1])\n",
    "            loss += loss_per_char.detach()\n",
    "\n",
    "        # We can't halt on EOS_token, because we have many sentences in a batch, and those are not obliged to be translated\n",
    "        # into sequences of exactly the same length. However, we want to stop printing the output after EOS_token.\n",
    "        output[:,char_index] = torch.argmax(decoder_output[:,:,char_index-1], dim=1)\n",
    "\n",
    "    return output, loss.detach() / target_length\n",
    "\n",
    "# Define the actual evaluation.\n",
    "def evaluateIters(encoder, decoder, print_every, process_every = 1):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0\n",
    "    num_of_pairs = 0\n",
    "    \n",
    "    checkpoint = torch.load(PATH + 'bytenet.eval.tar', map_location=DEVICE)\n",
    "    start_iter = checkpoint['iter']\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "    encoder.to(DEVICE)\n",
    "    decoder.to(DEVICE)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    print('Loaded the model from %sbytenet.eval.tar, which has already been trained for %d iterations.' % (PATH, start_iter))\n",
    "    criterion = nn.NLLLoss(reduction='sum')\n",
    "        \n",
    "    for batch, (input_tensor, target_tensor) in enumerate(bucketised_batches):\n",
    "        if batch % process_every != 0: continue\n",
    "        \n",
    "        if INSPECT_VISUALLY:\n",
    "            input_tensor  = input_tensor[:LIMIT_OUTPUT_PER_BATCH,:]\n",
    "            target_tensor = target_tensor[:LIMIT_OUTPUT_PER_BATCH,:]\n",
    "        \n",
    "        encoder_output = encoder(source=input_tensor)\n",
    "        decoder_output = decoder(target=target_tensor, encoder_output=encoder_output)\n",
    "        target_length = target_tensor.size(1) - 1\n",
    "        print_loss_total += criterion(decoder_output[:,:,:-1], target_tensor[:,1:]) / target_length\n",
    "        num_of_pairs += input_tensor.size(0)\n",
    "        \n",
    "        if batch % (print_every * process_every) == 0:\n",
    "            print('%s (%d %d%%) %d pairs processed so far; ' % (timeSince(start, num_of_pairs / DS_SIZE), batch, batch / len(bucketised_batches) * 100, num_of_pairs),end='')\n",
    "            print('the average loss so far: %.4f.' % (print_loss_total / num_of_pairs))\n",
    "        \n",
    "        if not INSPECT_VISUALLY: continue\n",
    "            \n",
    "        print('Bucket #{0}'.format(batch))\n",
    "        output, loss = evaluate(input_tensor, target_tensor, encoder, decoder, criterion)\n",
    "        output_TF = torch.argmax(decoder_output[:,:,:-1].detach(), dim=1)\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "        for source,target,result,result_TF in zip(input_tensor,target_tensor,output,output_TF):\n",
    "            print('> ',end='')\n",
    "            for char in itemgetter(*map(int,source))(all_letters): # Uses source as indices in the array all_letters.\n",
    "                if char not in [SOS_token,EOS_token,PSC_token]:   \n",
    "                    print(char,end='')\n",
    "                else: \n",
    "                    if char == EOS_token: break\n",
    "            print()\n",
    "            #print(list(map(int,source)))\n",
    "            \n",
    "            print('= ',end='')\n",
    "            for char in itemgetter(*map(int,target))(all_letters):\n",
    "                if char not in [SOS_token,EOS_token]:   \n",
    "                    print(char,end='')\n",
    "                else: \n",
    "                    if char == EOS_token: break\n",
    "            print()\n",
    "            #print(list(map(int,target)))\n",
    "            \n",
    "            print('<-TF ',end='')\n",
    "            for char in itemgetter(*map(int,result))(all_letters): # Uses source as indices in the array all_letters.\n",
    "                if char not in [SOS_token,EOS_token]:   \n",
    "                    print(char,end='')\n",
    "                else: \n",
    "                    if char == EOS_token: break\n",
    "            print()\n",
    "            #print(list(map(int,result)),end='\\n\\n')\n",
    "            \n",
    "            print('<+TF ',end='')\n",
    "            for char in itemgetter(*map(int,result_TF))(all_letters):\n",
    "                if char not in [SOS_token,EOS_token]:   \n",
    "                    print(char,end='')\n",
    "                else: \n",
    "                    if char == EOS_token: break \n",
    "            print()\n",
    "            #print(list(map(int,output_TF)),end='\\n\\n')\n",
    "        print()\n",
    "        \n",
    "    return print_loss_total, num_of_pairs\n",
    "\n",
    "# The computed training error implies teacher forcing (i.e., we only measure the loss in case at each step to predict the \n",
    "# next character the network is given the ground truth previous characters of the translation). Calculating the training\n",
    "# error without teacher forcing would take around a month on my hardware.\n",
    "with torch.no_grad():\n",
    "    encoder = CNN(n_letters, EMBED_DIM, RES_SETS, RES_BLOCKS, 'enc').to(DEVICE)\n",
    "    decoder = CNN(n_letters, EMBED_DIM, RES_SETS, RES_BLOCKS, 'dec').to(DEVICE)\n",
    "\n",
    "    print_loss_total, num_of_pairs = evaluateIters(encoder, decoder, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_of_pairs)\n",
    "print(print_loss_total/num_of_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVPNjHHh5tcz"
   },
   "outputs": [],
   "source": [
    "sum(p.numel() for p in encoder.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in decoder.parameters() if p.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AML Bytenet v5 (Train; DS100k).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
