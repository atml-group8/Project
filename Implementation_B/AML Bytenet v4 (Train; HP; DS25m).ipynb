{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29301,
     "status": "ok",
     "timestamp": 1587871210473,
     "user": {
      "displayName": "Igor Nikitin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRt0iOPsbCZ9N7PTqKQh4u9inTKGVTUY8ESsc7=s64",
      "userId": "16978142921003978451"
     },
     "user_tz": -60
    },
    "id": "e7R2X2YUzNPg",
    "outputId": "34e607fe-5e4a-46a4-c2bf-0e83e2bb97f2"
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### REQUIREMENTS ###\n",
    "####################\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "from itertools import islice # For reading only a part of the data file\n",
    "from collections import OrderedDict # For defining a variable-length nn.Sequential()\n",
    "from collections import defaultdict # Used in readData()\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import math # For math.ceil() in readLine()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABHoDZVsQ1F9"
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "### CONTROL CONSTANTS ###\n",
    "#########################\n",
    "\n",
    "# Major.\n",
    "TRAIN_DATA = 25*(10**6) # The number of bytes to train on.\n",
    "PATH = '.\\\\AML\\\\compr\\\\'\n",
    "PRED_IN = 100 # Section 5. The number of characters to predict from.\n",
    "PRED_OUT = 400 # Section 5. The number of characters to predict.\n",
    "LOAD_DATASET_FROM_FILE = True\n",
    "LOAD_MODEL_FROM_FILE = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "#DEVICE = torch.device(\"cuda\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "INSPECT_VISUALLY = True\n",
    "LIMIT_OUTPUT_PER_BATCH = 1\n",
    "\n",
    "# Minor.\n",
    "LOG = open(PATH + 'log', 'w', encoding='utf-8')\n",
    "LOGGING = False\n",
    "CUDA_LAUNCH_BLOCKING = 1\n",
    "START_ITER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-4R4p4ujHz4"
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### DATA-PREPROCESSING ###\n",
    "##########################\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Reading here is much easier. No cleaning, no sequences of different length.\n",
    "\n",
    "def bucketToBatch(bucket):\n",
    "    # Recall that we drop the last character in source.\n",
    "    source_tensor = torch.zeros(len(bucket), PRED_IN + PRED_OUT - 1, dtype=torch.int64, device=DEVICE)\n",
    "    target_tensor = torch.zeros(len(bucket), PRED_OUT, dtype=torch.int64, device=DEVICE)\n",
    "    for byte_str in range(len(bucket)):\n",
    "        for byte_pos, byte in enumerate(bucket[byte_str][0]):\n",
    "            source_tensor[byte_str][byte_pos] = byte\n",
    "        for byte_pos, byte in enumerate(bucket[byte_str][1]):\n",
    "            target_tensor[byte_str][byte_pos] = byte\n",
    "\n",
    "    return (source_tensor, target_tensor)\n",
    "\n",
    "def readData(enwik_file):\n",
    "    num_of_batch_files = 0\n",
    "    batches = []\n",
    "\n",
    "    enwik = open(PATH + '%s' % enwik_file, 'rb')\n",
    "    \n",
    "    tmp_bucket = []\n",
    "\n",
    "    # We read the binary file in chunks of chunk_size bytes.\n",
    "    chunk_size = PRED_IN + PRED_OUT # 500 in the paper\n",
    "    to_read = TRAIN_DATA // chunk_size\n",
    "\n",
    "    for seq in range(to_read):\n",
    "        if to_read >= 100 and seq % ((to_read // 100) * 5) == 0:\n",
    "            print(\"%d%% of the datafile was read.\" % (seq / (to_read // 100)))\n",
    "        tmp1 = enwik.read(PRED_IN)\n",
    "        tmp2 = enwik.read(PRED_OUT)\n",
    "        if len(tmp2) >= PRED_OUT: # If the enwik file hasn't ended yet.\n",
    "            # Thanks to a hint from Candidate #1041040: it is reasonable to approach this task similarly to translation.\n",
    "            # Here we take PRED_IN initial characters (instead of 1 SOS_token in translation), we need to predict \n",
    "            # PRED_OUT characters, and we use teacher forcing. Therefore, (numbering from 0) we are interested in outputs,\n",
    "            # corresponding to characters PRED_IN - 1 (the last character of the source) till PRED_IN + PRED_OUT - 2 \n",
    "            # (the 2nd to last character in the target; the character PRED_IN + PRED_OUT - 1 is the last one and isn't thus\n",
    "            # used in prediction).\n",
    "            tmp_bucket.append((tmp1+tmp2[:-1],tmp2))\n",
    "        if len(tmp_bucket) >= BATCH_SIZE:\n",
    "            batches.append(bucketToBatch(tmp_bucket))\n",
    "            del tmp_bucket[:]\n",
    "\n",
    "    if tmp_bucket:\n",
    "        batches.append(bucketToBatch(tmp_bucket))\n",
    "\n",
    "    torch.save(batches, PATH + 'enwik_batches_0')\n",
    "\n",
    "    return batches\n",
    "\n",
    "if not LOAD_DATASET_FROM_FILE:\n",
    "    readData('enwik9')\n",
    "batches = torch.load(PATH + 'enwik_batches_0', map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOqeJNfZxlBC"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### MODEL DEFINITION ###\n",
    "########################\n",
    "\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "\n",
    "# CONVOLUTION PARAMETER DEFINITIONS\n",
    "\n",
    "conv1x1 = {'kernel_size': 1, 'stride': 1, 'padding': 0, 'dilation': 1}\n",
    "conv1xk = {'kernel_size': 3, 'stride': 1, 'padding': 1} # Keep wdt odd and pad to (wdt+1)/2. On changing wdt MAKE SURE TO TEST masking in ResBlock::forward!!!\n",
    "\n",
    "CHANNEL_FACTOR = 2 # Do NOT touch this value!\n",
    "EMBED_DIM = 256\n",
    "\n",
    "# ENCODER & DECODER\n",
    "\n",
    "class ChannelLayerNorm(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelLayerNorm, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.trueBN = nn.Sequential(\n",
    "            nn.LayerNorm(channels)\n",
    "        ) # We trick LayerNorm into thinking that channels are actually characters. Thus, the normalisation is going over the channels. \n",
    "        # A similar trick for BatchNorm itself would require knowing the number of characters ahead of time, which is feasible for compression, but infeasible for translation.\n",
    "    def forward(self, batch):\n",
    "        batch = torch.transpose(batch,1,2) # Make 1 the dimension of characters and 2 the dimension of channels.\n",
    "        return torch.transpose(self.trueBN(batch),1,2)\n",
    "\n",
    "class MultiplicativeUnit(nn.Module):\n",
    "    def __init__(self, conv, type, channels):\n",
    "        super(MultiplicativeUnit, self).__init__()\n",
    "        self.type = type\n",
    "\n",
    "        self.MU_Path = nn.ModuleList([\n",
    "            nn.Sequential(OrderedDict([\n",
    "                ('conv_1', nn.Conv1d(channels, channels, **conv)),\n",
    "                ('norm_2', ChannelLayerNorm(channels)),\n",
    "                ('relu_1', nn.Sigmoid())\n",
    "            ])) for path in range(3)\n",
    "        ])\n",
    "\n",
    "        self.MU_Path.append(\n",
    "            nn.Sequential(OrderedDict([\n",
    "                ('conv_1', nn.Conv1d(channels, channels, **conv)),\n",
    "                ('norm_2', ChannelLayerNorm(channels)),\n",
    "                ('relu_1', nn.Tanh())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.type != '1x1':\n",
    "            for path in range(4):\n",
    "                c_out, c_in, wdt = self.MU_Path[path]._modules['conv_1'].weight.data.size()\n",
    "                self.MU_Path[path]._modules['conv_1'].weight.data[:,:,wdt//2+1:wdt] = torch.nn.Parameter(torch.zeros(c_out, c_in, wdt-wdt//2-1, device=DEVICE))\n",
    "\n",
    "        # Variable names correspond to the names in https://arxiv.org/pdf/1610.00527.pdf.\n",
    "        g1 = self.MU_Path[0](input)\n",
    "        g2 = self.MU_Path[1](input)\n",
    "        g3 = self.MU_Path[2](input)\n",
    "        u  = self.MU_Path[3](input)\n",
    "        return g1 * nn.Tanh()(g2 * input + g3 * u)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, dilation, type):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.type = type\n",
    "        conv1xk_dil = copy.deepcopy(conv1xk)\n",
    "        conv1xk_dil['padding'] *= dilation\n",
    "        conv1xk_dil['dilation'] = dilation\n",
    "\n",
    "        # Note that the size of padding in conv1xk is chosen so that the size of the output of the block is equal to the size of the input.\n",
    "        # Be careful about changing the order! Assumed to be so in forward()!!!\n",
    "        self.resblock = nn.Sequential(OrderedDict([\n",
    "            ('norm_1', ChannelLayerNorm(CHANNEL_FACTOR * channels)),\n",
    "            ('relu_1', nn.ReLU()),\n",
    "            ('conv1x1_1', nn.Conv1d(CHANNEL_FACTOR * channels, channels, **conv1x1)),\n",
    "            ('norm_2', ChannelLayerNorm(channels)),\n",
    "            ('relu_2', nn.ReLU()),\n",
    "            ('MU_1', MultiplicativeUnit(conv1xk_dil, '1xk', channels)),\n",
    "            ('MU_2', MultiplicativeUnit(conv1x1, '1x1', channels)),\n",
    "            ('conv1x1_2', nn.Conv1d(channels, CHANNEL_FACTOR * channels, **conv1x1))\n",
    "         ]))\n",
    "    \n",
    "    def forward(self, input):        \n",
    "        if LOGGING:\n",
    "            print('def ResBlock::forward(self, input)', file=LOG)\n",
    "            print(input.size(), file=LOG)\n",
    "            print(step1.size(), file=LOG)\n",
    "            print(step2.size(), file=LOG)\n",
    "            print(step3.size(), file=LOG, end='\\n')\n",
    "        \n",
    "        output = self.resblock(input)\n",
    "        return output + input\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, channels = EMBED_DIM):\n",
    "        super(CNN, self).__init__()\n",
    "        self.type = type\n",
    "        self.embed = nn.Embedding(input_dim, CHANNEL_FACTOR * channels)\n",
    "        self.channels = channels\n",
    "\n",
    "        layers = OrderedDict()\n",
    "        for res_set in range(RES_SETS):\n",
    "            for res_block in range(RES_BLOCKS):\n",
    "                layers['res_set_' + str(res_set) + '|res_block_' + str(res_block)] = ResBlock(channels, 2 ** res_block, type)\n",
    "\n",
    "        layers['fin|conv1x1'] = nn.Conv1d(CHANNEL_FACTOR * channels, channels, **conv1x1)\n",
    "        layers['fin|ReLU'] = nn.ReLU()\n",
    "        layers['fin|conv1xd'] = nn.Conv1d(channels, input_dim, **conv1xk)\n",
    "        layers['fin|logsoftmax'] = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.CNN = nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, source):\n",
    "        if LOGGING:\n",
    "            print('def CNN::forward(self, source, target, encoder_output) self.type == enc', file=LOG)\n",
    "            print('source', file=LOG)\n",
    "            print(source.size(), file=LOG, end='\\n')\n",
    "            #print(source, file=LOG)\n",
    "            \n",
    "        tmp = self.embed(source)\n",
    "        emb = torch.transpose(tmp, 1, 2)\n",
    "\n",
    "        c_out, c_in, wdt = self.CNN._modules['fin|conv1xd'].weight.data.size()\n",
    "        self.CNN._modules['fin|conv1xd'].weight.data[:,:,wdt//2+1:wdt] = torch.nn.Parameter(torch.zeros(c_out, c_in, wdt-wdt//2-1, device=DEVICE))\n",
    "        return self.CNN(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2WE1IAtx2sx"
   },
   "outputs": [],
   "source": [
    "################\n",
    "### TRAINING ###\n",
    "################\n",
    "\n",
    "# TIMING [COPY-PASTED FROM THE PYTORCH TUTORIAL]\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(s / percent - s))\n",
    "\n",
    "# Constants from the paper.\n",
    "\n",
    "ADAM_LEARNING_RATE = 0.00005\n",
    "RES_SETS = 6        # Number of sets of residual blocks\n",
    "RES_BLOCKS = 5      # Number of residual blocks per set\n",
    "\n",
    "# Define 1 GD step. Teacher Forcing\n",
    "def train_TF(input_tensor, target_tensor, decoder, decoder_optimizer, criterion):\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    decoder_output = decoder(source=input_tensor)\n",
    "\n",
    "    loss = criterion(decoder_output[:,:,PRED_IN-1:], target_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.detach() \n",
    "\n",
    "# Define the actual training.\n",
    "def trainIters(decoder, n_iters, print_every=1000, save_every=10000):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    start_iter = START_ITER\n",
    "\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), ADAM_LEARNING_RATE)\n",
    "\n",
    "    if LOAD_MODEL_FROM_FILE:\n",
    "        checkpoint = torch.load(PATH + 'bytenet_dec.train.tar', map_location=DEVICE)\n",
    "        start_iter = checkpoint['iter']\n",
    "        decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "        decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "\n",
    "        for param_group in decoder_optimizer.param_groups:\n",
    "            param_group['lr'] = ADAM_LEARNING_RATE\n",
    "\n",
    "        del checkpoint # Significantly reduces memory consumption \n",
    "        decoder.to(DEVICE)\n",
    "        decoder.train()\n",
    "\n",
    "        print('Loaded the model from %sbytenet_dec.train.tar, which has already been trained for %d iterations.' % (PATH, start_iter))\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(start_iter + 1, start_iter + n_iters + 1):\n",
    "        input_tensor, target_tensor = batches[iter % len(batches)]\n",
    "\n",
    "        loss = train_TF(input_tensor, target_tensor, decoder, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            real_iter = iter - start_iter\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, real_iter / n_iters), iter, real_iter / n_iters * 100, print_loss_avg),end=('\\n' if iter % save_every != 0 else ' '))\n",
    "\n",
    "        if iter % save_every == 0:\n",
    "            torch.save({\n",
    "                'iter': iter,\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "            }, PATH + 'bytenet_dec.train.tar')\n",
    "            print ('Saved!')\n",
    "\n",
    "decoder = CNN(256).to(DEVICE)\n",
    "trainIters(decoder, 1000000, print_every=600, save_every=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes Windows OS. Change to \"copy /y\" to \"cp -f\" on Linux.\n",
    "!copy /y {PATH + 'bytenet_dec.train.tar'} {PATH + 'bytenet_dec.eval.tar'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "### EVALUATION ###\n",
    "##################\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "# TIMING\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(s / percent - s))\n",
    "\n",
    "# EVALUATE\n",
    "\n",
    "EMBED_DIM = 200\n",
    "\n",
    "# Constants from the paper.\n",
    "\n",
    "RES_SETS = 6        # Number of sets of residual blocks\n",
    "RES_BLOCKS = 5      # Number of residual blocks per set\n",
    "\n",
    "# Define 1 evaluation step.\n",
    "def evaluate(input_tensor, target_tensor, decoder, criterion):\n",
    "    batch_size = target_tensor.size(0)\n",
    "    target_length = target_tensor.size(1)\n",
    "    loss = 0\n",
    "\n",
    "    output = torch.zeros(batch_size, PRED_IN + PRED_OUT - 1, dtype=torch.int64, device=DEVICE, requires_grad=False)\n",
    "    output[:,:PRED_IN] = input_tensor[:,:PRED_IN]\n",
    "\n",
    "    for char_index in range(PRED_OUT - 1):\n",
    "        decoder_output = decoder(source=output)\n",
    "        \n",
    "        loss_per_char = criterion(decoder_output[:,:,PRED_IN-1+char_index], target_tensor[:,char_index])\n",
    "        loss += loss_per_char.detach()\n",
    "\n",
    "        # We can't halt on EOS_token, because we have many sentences in a batch, and those are not obliged to be translated\n",
    "        # into sequences of exactly the same length. However, we want to stop printing the output after EOS_token.\n",
    "        output[:,PRED_IN+char_index] = torch.argmax(decoder_output[:,:,PRED_IN-1+char_index], dim=1)\n",
    "\n",
    "    return output, loss.detach() / target_length\n",
    "\n",
    "# Define the actual evaluation.\n",
    "def evaluateIters(decoder, print_every, process_every = 1):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0\n",
    "    num_of_pairs = 0\n",
    "    DS_SIZE = TRAIN_DATA / (PRED_IN + PRED_OUT)\n",
    "    \n",
    "    checkpoint = torch.load(PATH + 'bytenet_dec.eval.tar', map_location=DEVICE)\n",
    "    start_iter = checkpoint['iter']\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "    decoder.to(DEVICE)\n",
    "\n",
    "    decoder.eval()\n",
    "\n",
    "    print('Loaded the model from %sbytenet.eval.tar, which has already been trained for %d iterations.' % (PATH, start_iter))\n",
    "    criterion = nn.NLLLoss(reduction='sum')\n",
    "        \n",
    "    for batch, (input_tensor, target_tensor) in enumerate(batches):\n",
    "        if batch % process_every != 0: continue\n",
    "        \n",
    "        if INSPECT_VISUALLY:\n",
    "            input_tensor  = input_tensor[:LIMIT_OUTPUT_PER_BATCH,:]\n",
    "            target_tensor = target_tensor[:LIMIT_OUTPUT_PER_BATCH,:]\n",
    "        \n",
    "        decoder_output = decoder(source=input_tensor)\n",
    "        target_length = target_tensor.size(1)\n",
    "        print_loss_total += criterion(decoder_output[:,:,PRED_IN-1:], target_tensor) / target_length\n",
    "        num_of_pairs += input_tensor.size(0)\n",
    "        if batch % (print_every * process_every) == 0:\n",
    "            print('%s (%d %d%%) %d pairs processed so far; ' % (timeSince(start, num_of_pairs / DS_SIZE), batch, batch / len(batches) * 100, num_of_pairs),end='')\n",
    "            print('the average loss so far: %.4f.' % (print_loss_total / num_of_pairs))\n",
    "        \n",
    "        if not INSPECT_VISUALLY: continue\n",
    "            \n",
    "        print('Bucket #{0}'.format(batch))\n",
    "        output, loss = evaluate(input_tensor, target_tensor, decoder, criterion)\n",
    "        output_TF = torch.argmax(decoder_output[:,:,PRED_IN-1:].detach(), dim=1)\n",
    "        \n",
    "        for source,target,result,result_TF in zip(input_tensor,target_tensor,output,output_TF):\n",
    "            print('> ',end='')\n",
    "            print(''.join([chr(x) for x in input_tensor[0,:PRED_IN]]))\n",
    "            #print(list(map(int,source)))\n",
    "            \n",
    "            print('= ',end='')\n",
    "            print(''.join([chr(x) for x in target]))\n",
    "            #print(list(map(int,target)))\n",
    "            \n",
    "            print('<-TF ',end='')\n",
    "            print(''.join([chr(x) for x in result]))\n",
    "            #print(list(map(int,result)),end='\\n\\n')\n",
    "            \n",
    "            print('<+TF ',end='')\n",
    "            print(''.join([chr(x) for x in result_TF]))\n",
    "            #print(list(map(int,output_TF)),end='\\n\\n')\n",
    "            \n",
    "            print('\\n###\\n###\\n###\\n')\n",
    "        \n",
    "    return print_loss_total, num_of_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder = CNN(256).to(DEVICE)\n",
    "    print_loss_total, num_of_pairs = evaluateIters(decoder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in decoder.parameters())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPHxDq1AzlVmtF58xJbunIa",
   "collapsed_sections": [],
   "name": "AML Bytenet v4 (Train; HP; DS1m).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
